# 机器学习基础 线性回归

[TOC]

# 1. 最小二乘法

## 1.1 数据集定义

​		给定数据集 $D=\{(x_1,y_1),\cdots,(x_N,y_N)\}$，其中 $x_i\in \mathbb{R}^p$，$y_i\in \mathbb{R}$，$i=1,2,\cdots,N$。对于输入样本定义为：
$$
\begin{align}
    X=(x_1\; x_2\; \cdots \; x_N)^T=
\begin{bmatrix}
    x_1^T \\
    x_2^T \\
    \vdots \\
    x_N^T
\end{bmatrix}=
\begin{bmatrix}
    x_{11} & x_{12} & \cdots & x_{1p} \\
    x_{21} & x_{22} & \cdots & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{N1} & x_{N2} & \cdots & x_{Np}
\end{bmatrix}_{N \times p}.
\end{align}\tag{1}
$$
​		定义输出为：
$$
\begin{align}

  Y=

\begin{bmatrix}

  y_1 \\

  y_2 \\

  \vdots \\

  y_N

\end{bmatrix}_{N \times 1}.

\end{align}\tag{2}
$$

## 1.2 最小二乘的矩阵推导

​		最小二乘法的损失函数定义为：
$$
\begin{align}
    L(w) &= \sum_{i=1}^N \lVert w^T x_i - y_i \rVert^2 \\[4pt]
    &= \sum_{i=1}^N (w^T x_i - y_i)^2 \\[4pt]
    &= 
    (w^Tx_1-y_1\; w^Tx_2-y_2\; \cdots\; w^Tx_N-y_N)
    \begin{bmatrix}
    w^T x_1 - y_1 \\[4pt]
    w^T x_2 - y_2 \\[4pt]
    \vdots \\[4pt]
    w^T x_N - y_N
    \end{bmatrix},
\end{align}\tag{3}
$$
第一项可继续化简：
$$
\begin{align}
    &=(w^Tx_1\; w^Tx_2\; \cdots\; w^Tx_N) - (y_1\; y_2\; \cdots\; y_N)\\[4pt]
    &=w^T(x_1 \; x_2 \; \cdots x_N) - (y_1\; y_2\; \cdots\; y_N)\\[4pt]
    &=w^T X^T - Y^T.
\end{align}\tag{4}
$$
​		因为 $x_i^T=[x_{i1}\; x_{i2}\;\cdots \;x_{ip}]$ 是 $1\times p$ 的行向量，那么 $x_i$ 是一个 $p\times 1$ 的列向量。$w$ 作为权重系数，是一个  $p\times 1$ 的列向量，转置后就变成了 $1\times p$ 的行向量。故第二项可以写成如下的矩阵形式：
$$
\begin{align}
    Xw-Y
\end{align}.\tag{5}
$$
​		因此，可以将损失函数表示为：
$$
\begin{align}
    L(w)&=(w^T X^T - Y^T)(Xw-Y)\\[4pt]
    &=w^T X^T X w - Y^T X w - w^T X^TY + Y^TY\\[4pt]
    &=w^T X^T X w - 2Y^T X w + Y^TY.
\end{align}\tag{6}
$$
​		最小二乘的目标是求解损失函数 $L(w)$ 的最小值，即：
$$
\begin{align}
    \hat w = \underset{w}{\text{argmin}}\; L(w).
\end{align}\tag{7}
$$
​		对矩阵求导有：
$$
\begin{align}
    \frac{\partial{L(w)}}{\partial{w}}&=2X^TXw - 2X^TY \overset{\triangle}{=} 0\\[4pt]
    &\Rightarrow X^TXw = X^TY\\[4pt]
    &\Rightarrow w = (X^TX)^{-1}X^TY,
\end{align}\tag{8}
$$
其中，$X^TX=X^{+}$ 表示伪逆。

## 1.3 最小二乘的几何解释

​		公式 (3) 可以理解为线性回归总的误差，假设拟合的是二维平面上的一条直线，那么第一种几何解释就是每个样本点的拟合值到真实值的距离，可视化如图 1 所示。

<img src="D:\0保研\机器学习\线性回归_图1.png" alt="线性回归_图1" style="zoom:50%;" />

<center>
    图1 基于距离的几何解释.
</center>

​		我们假设拟合的直线是 $f(w)=Xw$，如果横着看 $X$，每一行可以理解为是一个样本。如果竖着看，那么每一列都是一个列向量，这些列向量都存在于 $N$ 维的空间中。

​		当列向量线性无关时，$p$ 个列向量会张成一个 $p$ 维的子空间。比如，两个列向量在 3 维空间中张成了一个平面。在一般情况下，观测向量 $Y$ 不会完美地被直线拟合，故 $Y$ 不会存在于 $X$ 的列空间中。

​		最小二乘法的另一种几何解释即为，在 $p$ 维子空间中寻找一个向量 $f(w)$，使得 $f(w)$ 与 $Y$ 的距离最小。显然是当 $f(w)$ 为 $Y$ 的投影时距离最小，则满足 $Y-f(w)$ 与 $p$ 维子空间的基向量正交，故求解：
$$
\begin{align}
    &X^T(Y-f(w))=0\\[4pt]
    &\Rightarrow X^TY = X^T Xw\\[4pt]
    &\Rightarrow w = (X^TX)^{-1}X^TY.
\end{align}\tag{9}
$$
<img src="D:\0保研\机器学习\线性回归_图2.png" alt="线性回归_图2" style="zoom:50%;" />

<center>
    图2 基于子空间的几何解释.
</center>

## 1.4 概率视角下的最小二乘估计

​		假设 $y=w^Tx+\epsilon$，这里的 $x$ 是 $p\times 1$ 维的列向量，$\epsilon$ 是数据中的噪声，且 $\epsilon\sim \mathcal{N}(0,\sigma^2)$。因此，对于 $y$ 有：
$$
\begin{align}
    y|x;w \sim \mathcal{N}(w^Tx,\sigma^2).\tag{10}
\end{align}
$$
​		因为 $y$ 也服从正态分布，接下来可以用极大似然估计求解 $w$。假设 $N$ 个样本之间独立同分布，有以下似然函数：
$$
\begin{align}

  \mathcal{L}(w)=\text{log}\; P(Y|X,w)=\text{log}\; \prod_{i=1}^N P(y_i|x_i,w) 

  = \sum_{i=1}^N \text{log}\; P(y_i|x_i,w),\tag{11}

\end{align}
$$
其中，正态分布的概率密度函数：
$$
\begin{align}

  P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma}\text{exp}\{-\frac{(y-w^Tx)^2}{2\sigma^2}\}

\end{align},\tag{12}
$$
代回似然函数中可得：
$$
\begin{align}

  \mathcal{L}(w)&=\sum_{i=1}^N \text{log}\; P(y_i|x_i,w)\\[4pt]

  &=\sum_{i=1}^N \text{log}\; \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}\}\\[4pt]

  &=\sum_{i=1}^N (\text{log}\; \frac{1}{\sqrt{2\pi}\sigma} - \frac{(y_i-w^Tx_i)^2}{2\sigma^2}).\tag{13}

\end{align}
$$
​		极大似然估计有：
$$
\begin{align}

  \hat w &= \underset{w}{\text{argmax}}\; \mathcal{L}(w)\\[4pt]

  &= \underset{w}{\text{argmax}}\; - \frac{1}{2\sigma^2}(y_i-w^Tx_i)^2\\[4pt]

  &= \underset{w}{\text{argmin}}\; (y_i-w^Tx_i)^2.\tag{14}

\end{align}
$$
​		由此可见，最小二乘估计等价于噪声服从正态分布的极大似然估计。

# 2. 正则化

## 2.1 L1 范数与 L2 范数

​		L1 正则化和 L2 正则化可以看做是损失函数的惩罚项。所谓惩罚是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用 L1 正则化的模型建叫做 Lasso 回归，使用 L2 正则化的模型叫做 Ridge 回归。其中，Lasso 回归的形式如下：
$$
\underset{w}{\text{argmin}}\ \frac{1}{2n}\lVert Xw-y \rVert_2^2+\alpha\lVert w \rVert_1.\tag{15}
$$
​		Ridge 回归的形式如下：
$$
\underset{w}{\text{argmin}}\ \lVert Xw-y \rVert_2^2+\alpha\lVert w \rVert_2^2.\tag{16}
$$

## 2.2 正则化的作用

​		如图 3 所示，其目标在于在满足正则化约束的前提下，让损失函数最小。也就是说，让误差函数的等值线（椭圆）尽可能小，但又不能越过正则化约束线（菱形或圆），交点即为最优解。

​		由于菱形有“尖角”正好在坐标轴上，故 L1 范数得到的最优解，恰好可能落在某条坐标轴上。进而某些参数会被压缩为 0，相当于自动做了“特征选择”。

​		L2 范数的作用是改善过拟合。过拟合是：模型训练时候的误差很小，但是测试误差很大，也就是说模型复杂到可以拟合到所有训练数据，但在预测新的数据的时候，结果很差。相对于 L1 范数，L2 范数可以使得 $w$ 的每个元素都很小，都接近于 0。

<img src="D:\0保研\机器学习\线性回归_图3.png" alt="线性回归_图3" style="zoom: 25%;" />

<center>
    图3 L1 正则化比 L2 正则化更容易得到稀疏解.
</center>

## 2.3 Lasso 回归的求解

### 2.3.1 L-Lipschitz 条件

​		较为常见的是使用近端梯度下降法对 Lasso 进行求解，近端梯度下降要求如下形式的 $\nabla f$ 满足 L-Lipschitz 条件
$$
\min_x\ \ f(x)+h(x)\tag{17}
$$
​		因此，首先证明 Lasso 表达式（公式 15）的第一部分的梯度满足 L-Lipschitz 条件，即证明线性回归模型的均方误差满足 L-Lipschitz 条件。这里引用知乎用户 cielo 的回答，说得非常形象。

>  L-Lipschitz 连续，要求函数图像的曲线上任意两点连线的斜率一致有界，就是任意的斜率都小于同一个常数，这个常数就是 L-Lipschitz 常数。
>
> 从局部看：我们可以取两个充分接近的点，如果这个时候斜率的极限存在的话，这个斜率的极限就是这个点的导数。也就是说函数可导，又是 L-Lipschitz 连续，那么导数有界。反过来，如果可导函数，导数有界，可以推出函数 L-Lipschitz 连续。
>
> 从整体看：L-Lipschitz 连续要求函数在无限的区间上不能有超过线性的增长，所以 $x^2$, $e^x$ 这些函数在无限区间上不是 L-Lipschitz 连续的。

​		这里可以通过维基百科的示意图进行辅助解释：

![线性回归_图4](D:\0保研\机器学习\线性回归_图4.webp)

<center>
    图4 维基百科上对 L-Lipschitz 条件的示意图
</center>

### 2.3.2 L-Lipschitz 条件的证明

​		定义线性回归的均方误差函数为：
$$
f(w)=\frac{1}{2n}\lVert y-Xw \rVert^2_2.\tag{18}
$$
​		Lasso 回归的 $f(x)$ 部分与线性回归的均方误差函数相同，下面证明该部分满足 L-Lipschitz 条件，定义：
$$
F(x_1,x_2)=\lVert \nabla f(x_1) - \nabla f(x_2) \rVert_2^2 - L\lVert x_1-x_2 \rVert_2^2.\tag{19}
$$
​		首先求解 $\nabla f(w)$，对式 (18) 进行展开得：
$$
f(w)=\frac{1}{2n}(y-Xw)^T(y-Xw).\tag{20}
$$
​		根据矩阵求导法则：
$$
\frac{∂}{∂w}(a^Ta)=2a^T\frac{∂a}{∂w}.\tag{21}
$$
​		令 $a=(y-Xw)$，对式 (18) 求导后转置，故 $\nabla f(w)$ 为：
$$
\nabla f(w)=-\frac{1}{n}X^T(y-Xw),\tag{22}
$$
代入式 (19) 得：
$$
\begin{align}
F(w_1,w_2)&=\lVert \frac{1}{n}X^T(Xw_1-y) +\frac{1}{n}X^T(Xw_2+y) \rVert_2^2-L\lVert w_1-w_2 \rVert_2^2\\[4pt]
&=\frac{1}{n^2}\lVert X^TX(w_1-w_2) \rVert_2^2 - L\lVert w_1-w_2 \rVert_2^2
\end{align}.\tag{23}
$$
​		现在要证明 $F(w_1,x_2)\le0$，则要证明下式：
$$
\frac{\lVert X^TX(w_1-w_2) \rVert_2^2}{n^2\lVert w_1-w_2 \rVert_2^2}\le L,\tag{24}
$$
记 $A = X^T X$，实际上是证明对于任意的 $x$，下式有界：
$$
\frac{\lVert Ax \rVert_2^2}{\lVert x \rVert_2^2}.\tag{25}
$$
​		由于 $A$ 是实对称矩阵，因此可以被对角化，即存在一个正交矩阵 $Q$ 和一个对角矩阵 $\Lambda$，使得：
$$
A=Q\Lambda Q^T,\tag{26}
$$
其中，$Q$ 是 $A$ 的特征向量，$\Lambda$ 的对角线元素是 $A$ 的特征值。

​		对于特征向量的正交性，如果 $v_i$ 和 $v_j $ 是两个不同的特征向量，分别对应不同的特征值 $\lambda_i \neq \lambda_j$，满足：
$$
Av_i=\lambda _iv_i,\quad Av_j=\lambda _jv_j,\tag{27}
$$
则它们之间满足正交关系：
$$
v_i^T v_j = 0.\tag{28}
$$
​		除此之外，可以进一步选择特征向量使其单位化，即 $\lVert v_i\rVert = 1$，使得它们形成一个标准正交基。

​		基于此，$x$ 可以表示为 $A$ 的特征向量的线性组合：
$$
x=\sum_{i=1}^n{k_iv_i},\tag{29}
$$
其中，$v_i$ 是 $A$ 的特征向量，$k_i$ 是系数。

​		因此，$Ax$ 可以化简为：
$$
Ax=A(\sum_{i=1}^n{k_iv_i})=\sum_{i=1}^n{k_iAv_i}=\sum_{i=1}^n{k_i\lambda_iv_i},\tag{30}
$$
其中，$\lVert Ax \rVert_2^2$ 可以化简为：
$$
\lVert Ax \rVert_2^2=\lVert \sum_{i=1}^n{k_i\lambda_iv_i} \rVert_2^2=\sum_{i=1}^n(k_i\lambda_i)^2\lVert v_i\rVert_2^2=\sum_{i=1}^n(k_i\lambda_i)^2.\tag{31}
$$
​		同理，$\lVert x \rVert_2^2$可以化简为：
$$
\lVert x \rVert_2^2=\sum_{i=1}^{n}{k_i}^2.\tag{32}
$$
​		综上所述，可以将式 (25) 化简为：
$$
\frac{\lVert Ax \rVert_2^2}{\lVert x \rVert_2^2}=\frac{\sum_{i=1}^n(k_i\lambda_i)^2}{\sum_{i=1}^{n}{k_i}^2}.\tag{33}
$$
​		式 (33) 是一个加权平均式，因此有如下结论：
$$
\min_i{(\lambda_i^2)}\le \frac{\sum_{i=1}^n(k_i\lambda_i)^2}{\sum_{i=1}^{n}{k_i}^2}\ \le\max_i(\lambda_i^2).\tag{34}
$$
​		因此式 (25) 有界，即：
$$
\frac{\lVert X^TX(w_1-w_2) \rVert_2^2}{\lVert w_1-w_2 \rVert_2^2}\le\max_i(\lambda_i^2).\tag{35}
$$
​		接下来求解符合条件的 $L$：
$$
\begin{align}
F(w_1,w_2)&=\frac{1}{n^2}\lVert X^TX(w_1-w_2) \rVert_2^2 - L\lVert w_1-w_2 \rVert_2^2\\[4pt]
&=L\lVert w_1-w_2 \rVert_2^2(\frac{\lVert X^TX(w_1-w_2) \rVert_2^2}{n^2L\lVert w_1-w_2 \rVert_2^2}-1)\\[4pt]
&\le L\lVert w_1-w_2 \rVert_2^2(\frac{1}{n^2L}\max_i(\lambda_i^2)-1)
\le 0.
\end{align}\tag{36}
$$
​		故可得 L-Lipschitz 常数的上界：
$$
L\ge \frac{\max_i(\lambda_i^2)}{n^2}.\tag{37}
$$

## 2.4 近端梯度下降

### 2.4.1 近端梯度下降的定义

​		在上一节已经证明了 Lasso 的 $\nabla f$ 满足 L-Lipschitz 条件，因此可以使用近端梯度下降算法进行求解。实际上，近端梯度下降的本质可以视为先对光滑的函数 $f(x)$ 进行梯度下降，再对正则项进行隐性的梯度下降。

​		梯度的 L-Lipschitz 连续性要求梯度的变化不能过于剧烈，保证了存在合适的步长，使得 $f(x)$ 沿着梯度方向前进这个步长可以使函数值下降，即 descent lemma，这可用于证明算法收敛性。

​		近端梯度算法（Proximal Gradient Descent, PGD）常用来解决如下形式的优化问题：
$$
\min_x\ \ f(x)+h(x).\tag{38}
$$
​		近端梯度下降的 $h(x)$ 有以下几种形式：

1. $h(x)=0$：则为一般的梯度下降算法。

2. $h(x)=I_C(x)$：则称为投影梯度下降（Projected gradient descent），其中：
   $$
   I_C(x)=\begin{cases}0,&x\in C,\\[4pt]
   \infty,&x\notin C.\tag{39}
   \end{cases}
   $$

3. $h(x)=\lambda\lVert x\rVert_1$：则被称为迭代收缩阈值算法（Iterative Shrinkage Thresholding Algorithm, ISTA），Lasso 回归为该种类型。

### 2.4.2 迭代方法

​		对于函数 $f(x)$，在 $x_k$ 附近进行二阶泰勒展开得：
$$
\hat f(x)\simeq f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{1}{2}(x-x_k)^T\nabla ^2f(x_k)(x-x_k),\tag{40}
$$
上式中，$\nabla ^2f(x_k)$ 为 Hesse 矩阵，并且正定，L-Lipschitz 常数是 Hesse 矩阵的上界，在优化算法中常这样进行简化，故上式可以化简为如下形式：
$$
\hat f(x)\le f(x_k)+\lang \nabla f(x_k),(x-x_k)\rang+\frac{L}{2}\lVert (x-x_k)\rVert _2^2,\tag{41}
$$
上述化简中，向量内积写法为：$\nabla f(x_k)^T(x-x_k)=\lang\nabla f(x_k),x-x_k\rang$，范数平方写法为：$\lVert x-x_k\rVert_2^2=(x-x_k)^T(x-x_k)$。此外，由于 $\nabla f$ 满足 L-Lipschitz 条件，故 $\lVert \nabla^2f(x)\rVert_2\le L,\ \forall x$，综合这些条件即可化简上式。

​		进一步化简有：
$$
\begin{align}
\hat f(x)&\le f(x_k)+\lang \nabla f(x_k),\ (x-x_k)\rang+\lang \frac{L}{2}(x-x_k),\ x-x_k \rang\\[4pt]
&=f(x_k)+\lang \nabla f(x_k)+\frac{L}{2}(x-x_k),\ x-x_k \rang\\[4pt]
&=f(x_k)+\frac{L}{2}\lang\frac{2}{L} \nabla f(x_k)+(x-x_k),\ x-x_k \rang\\[4pt]
&=f(x_k)+\frac{L}{2}\lang x-x_k+\frac{1}{L}\nabla f(x_k)+ \frac{1}{L}\nabla f(x_k),\ x-x_k+\frac{1}{L}\nabla f(x_k)-\frac{1}{L}\nabla f(x_k)\rang\\[4pt]
&=f(x_k)+\frac{L}{2}\lVert x-x_k+\frac{1}{L}\nabla f(x_k)\rVert_2^2-\frac{1}{2L}\lVert \nabla f(x_k) \rVert_2^2\\[4pt]
&=\frac{L}{2}\lVert x-(x_k-\frac{1}{L}\nabla f(x_k)) \rVert_2^2+\text{const},
\end{align}\tag{42}
$$
上式中，第一步到第二步的化简利用了内积合并的线性性质：$\lang a,\ c\rang+\lang b,\ c\rang=\lang a+b,\ c\rang$，第四步到第五步的化简可以令 $a=x-x_k+\frac{1}{L}\nabla f(x_k)$，则 $\lang a+\delta,\ a-\delta \rang=\lVert a \rVert^2-\lVert \delta \rVert^2$，代入后乘上因子 $\frac{L}{2}$ 即可得到第五步。最后化简结果中的 $\text{const}$ 是常数。

​		显然，式 (42) 的最小值在如下 $x_{k+1}$ 获得：
$$
x_{k+1}=x_k-\frac{1}{L}\nabla f(x_k).\tag{43}
$$
​		下面证明通过迭代上式可以使 $f(x)$ 逐步下降。

### 2.4.3 Descent Lemma 的证明

​		对于函数 $f(x)$，由定积分的定义可得：
$$
f(x_2)-f(x_1)=\int_{x_1}^{x_2}f'(x)dx.\tag{44}
$$
​		设 $f(x)$ 二次连续可微，$x_k\in\R^n$，令 $g(t)=f(x_k+t(x-x_k))$，则：
$$
\begin{align}
g(0)&=f(x_k),
\\[3pt]g(1)&=f(x).
\end{align}\tag{45}
$$
​		因此，由定积分的定义可得：
$$
f(x)-f(x_k)=g(1)-g(0)=\int_0^1{g'(t)dt}.\tag{46}
$$
​		对于 $g'(t)$ 有：
$$
\begin{align}
g'(t)&=\frac{\part f(x_k+t(x-x_k))}{\part t},\\[4pt]
&=\nabla f(x_k+t(x-x_k))^T(x-x_k),
\end{align}\tag{47}
$$
代入式 (46) 可得：
$$
\begin{align}
f(x)-f(x_k)&=\int_0^1{\nabla f(x_k+t(x-x_k))^T(x-x_k)}dt,\\[4pt]
&=\int_0^1[\nabla f(x_k+t(x-x_k))-\nabla f(x_k)+\nabla f(x_k)]^T(x-x_k)dt,\\[4pt]
&=\int_0^1\nabla f(x_k)^T(x-x_k)dt+\int_0^1[\nabla f(x_k+t(x-x_k))-\nabla f(x_k)]^T(x-x_k)dt,
\end{align}\tag{48}
$$
对上式使用如下内积性质进行放缩：
$$
x^Ty=\lVert x \rVert \cdot \lVert y \rVert \cos\theta\le \lVert x \rVert \cdot \lVert y \rVert.\tag{49}
$$
​		故式 (48) 可以放缩为如下形式：
$$
f(x)-f(x_k)\le\int_0^1\nabla f(x_k)^T(x-x_k)dt+\int_0^1\lVert\nabla f(x_k+t(x-x_k))-\nabla f(x_k)\rVert_2\cdot \lVert x-x_k\rVert _2dt.\tag{50}
$$
​		接下来对上式使用 L-Lipschitz 常数进行放缩，可得到：
$$
f(x)-f(x_k)\le\int_0^1\nabla f(x_k)^T(x-x_k)dt+\lVert x-x_k\rVert _2\int_0^1L\lVert t(x-x_k)\rVert _2dt,\tag{51}
$$
对上式进行定积分，可化简为：
$$
f(x)-f(x_k)=\nabla f(x_k)^T(x-x_k)+\frac{L}{2}\lVert (x-x_k)\rVert _2^2,\tag{52}
$$
令
$$
\hat f(x)=f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{L}{2}\lVert (x-x_k)\rVert _2^2,\tag{53}
$$
则有 $f(x)\le\hat f(x)$，当 $x=x_k$ 时等号成立，该不等式被称为 descent lemma。

​		由于 $f(x)$在$x=x_k-\frac{1}{L}\nabla f(x_k)$ 处取得最小值，因此以下不等式成立：
$$
\hat f(x_k-\frac{1}{L}\nabla f(x_k))\le \hat f(x_k).\tag{54}
$$
​		根据 descent lemma 的推导有 $f(x)\le\hat f(x)$ 恒成立，因此以下不等式也成立：
$$
f(x_k-\frac{1}{L}\nabla f(x_k))\le \hat f(x_k-\frac{1}{L}\nabla f(x_k)),\tag{55}
$$
且当 $x=x_k$ 时，由式 (53) 可知 $f(x_k)=\hat f(x_k)$，因此：
$$
f(x_k-\frac{1}{L}\nabla f(x_k))\le \hat f(x_k-\frac{1}{L}\nabla f(x_k))\le\hat f(x_k)=f(x_k).\tag{56}
$$
​		综上所述，可以通过迭代下式使得 $f(x)$ 逐步下降：
$$
x_{k+1}=x_k-\frac{1}{L}\nabla f(x_k).\tag{57}
$$

### 2.4.4 求解 Lasso

​		上面的证明和推导都是针对 Lasso 的第一部分，Lasso 相对于线性回归只是加上了 L1 范数，故将式 (57) 应用在 Lasso 回归上，可得到如下一阶近似的迭代式：
$$
x_{k+1}=\underset{x}{\arg\min}\ \frac{L}{2}\lVert x-(x_k-\frac{1}{L}\nabla f(x_k)) \rVert_2^2+\lambda\lVert x\rVert_1,\tag{58}
$$
令 $z=x_k-\frac{1}{L}\nabla f(x_k)$，$L=\frac{1}{t}$ 其实际上为 $f(x)$ 的梯度下降形式，可以将上式转换成如下形式：
$$
x_{k+1}=\underset{x}{\arg\min}\ \frac{1}{2}\lVert x-z \rVert_2^2+\lambda t\lVert x\rVert_1,\tag{59}
$$
上式也被称为近端算子（proximal operator），记为：
$$
prox_{t,h(\cdot)}(z)=\underset{x}{\arg\min}\ \frac{1}{2}\lVert x-z\rVert_2^2+t\cdot h(x).\tag{60}
$$
​		总体而言，首先对可导部分使用一阶泰勒展开和二次近似，然后再对整体目标函数应用近端操作。即先走一小步梯度下降，然后用近端算子来处理非光滑部分。特别地，当 $h(x)=\lambda\lVert x\rVert_1$时，近端算子有封闭解，称为 Soft Thresholding：
$$
[prox_{t\lambda \lVert \cdot \rVert_1}(z)]_i=
\begin{cases}
z_i-t\lambda,&z_i>t\lambda,\\[4pt]
0,&|z_i|\le t\lambda,\\[4pt]
z_i+t\lambda,&z_i<-t\lambda,
\end{cases}\tag{61}
$$
更简洁地，可以写为：
$$
prox_{t\lambda\lVert\cdot \rVert_1}(z)=\text{sign}(z)\cdot \max (|z|-t\lambda,\ 0).\tag{62}
$$
​		对于式 (17) 的问题，近端梯度下降算法的一般迭代过程是先对 $f(x)$ 进行梯度下降求得 $z^{(k)}$，然后代入近端算子 $prox_{t,h(\cdot)}(z^{(k)})$，然后迭代求解：
$$
\begin{align}
z^{(k)}&=x^{(k)}-t\nabla f(x^{(k)}),\\[4pt]
x^{(k+1)}&=prox_{t,h(\cdot)}(z^{(k)}).
\end{align}\tag{63}
$$

### 2.4.5 具体求解例子

​		假设数据设置有：
$$
X=
\begin{bmatrix}
1&3\\[4pt]
3&4
\end{bmatrix},\quad
y=\begin{bmatrix}
1\\[4pt]2
\end{bmatrix}\quad
w^{(0)}=\begin{bmatrix}
0\\[4pt]0
\end{bmatrix},\quad
\lambda=0.1,\quad t=0.01.\tag{64}
$$
​		第一步需要计算梯度 $\nabla f(w)$：
$$
\begin{align}
f(w)&=\frac{1}{2n}\lVert Xw-y \rVert^2\\[4pt]&\Rightarrow \nabla f(w)=-\frac{1}{n}X^T(y-Xw)\\[4pt]
&\Rightarrow \nabla f(w^{(0)})=
\begin{bmatrix}
-7\\[4pt]-10
\end{bmatrix}.
\end{align}\tag{65}
$$
​		第二步进行梯度下降和近端操作：
$$
z^{(0)}=w^{(0)}-t\nabla f(w^{(0)})=\begin{bmatrix}
0.07\\[4pt]0.10
\end{bmatrix},\tag{66}
$$
对每一维使用 Soft-thresholding：
$$
\begin{align}
0.07\rightarrow\max(0.07-0.001,\ 0)=0.069\\[4pt]
0.10\rightarrow\max(0.10-0.001,\ 0)=0.099,\tag{67}
\end{align}
$$
因此第一轮更新的权重为：
$$
w^{(1)}=prox(z^{(0)})=
\begin{bmatrix}
0.069\\[4pt]0.099
\end{bmatrix}.\tag{68}
$$

# 3. 最大后验估计

​		最大后验估计是从贝叶斯的角度上的一种求解方式。假设 $w$ 有一个先验，满足 $w\sim \mathcal N(0,\sigma_0^2)$，噪声 $\epsilon\sim \mathcal N(0,\sigma^2)$，目标变量 $y\sim \mathcal N(w^Tx,\sigma^2)$。根据高斯分布的概率密度函数可写出似然函数和 $w$ 的先验分布：

$$
\begin{align}
p(y|w)&=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}\},
\end{align}\tag{69}
$$
取对数并忽略常数：
$$
\log\ p(y|w)\propto -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-w^Tx_i)^2=-\frac{1}{2\sigma^2}\lVert Xw-y \rVert^2.\tag{70}
$$
​		先验项有：
$$
p(w)=\frac{1}{(2\pi \sigma_0^2)^{d/2}}\exp\{-\frac{\lVert w \rVert^2}{2\sigma_0^2}\},\tag{71}
$$
取对数并忽略常数：
$$
\log \ p(w)\propto -\frac{1}{2\sigma_0^2} \lVert w \rVert^2.\tag{72}
$$
​		最大后验等价于最小负对数后验：
$$
\begin{align}
\hat w&=\underset{w}{\text{argmax}}\ \log\ p(y|w)+\log \ p(w)\\[4pt]
&=\underset{w}{\text{argmin}}\ \frac{1}{2\sigma^2}\lVert Xw-y \rVert^2 + \frac{1}{2\sigma_0^2} \lVert w \rVert^2,
\end{align}\tag{73}
$$
乘上 $2\sigma^2$ 不影响最小化目标：
$$
\hat w=\underset{w}{\text{argmin}}\ \lVert Xw-y \rVert^2 + \lambda \lVert w \rVert^2,\quad \lambda=\frac{\sigma^2}{\sigma_0^2}.\tag{74}
$$
​		回顾最小二乘估计的求解结果，如果样本量 $N$ 没有远大于维度 $p$ 的时候，$X^TX$ 往往不可逆，也就是直观上理解的“过拟合”。在这种情况下，有三种方法：增加样本量、特征选择/降维、正则化。其中，正则化的框架为：

$$
J(w)=L(w)+\lambda P(w).\tag{75}
$$
​		在岭回顾中，其克服过拟合的思想为，一个半正定矩阵加上对角矩阵为正定矩阵，必定可逆。在上式框架中令 $P(w)=w^Tw$，经最小二乘估计可求出 $w$ 为：
$$
\hat w=\underset{x}{\arg \min}\ {J(w)}=(X^TX+\lambda I)^{-1}X^TY,\tag{76}
$$
上式等价于式子 (16)，观察式子 (16) 和式子 (74)，因此可以得出结论：噪声和先验为高斯分布的最大后验估计等价于正则化的最小二乘估计。

# 参考

[1] [Machine Learning基础：正则化项（Regularization）](https://www.jianshu.com/p/eb6fd3b5c001)

[2] [【西瓜书】 第11章 特征选择与稀疏学习](https://www.jianshu.com/p/0987a8841573)

[3] [近端梯度(Proximal Gradient)下降算法的过程以及理解|ISTA算法|LASSO问题 - 红泥小火炉的文章 - 知乎](https://zhuanlan.zhihu.com/p/530839064)

[4] [Lasso回归与近端梯度下降推导与实现Posted by Welt Xing on September 30, 2021](https://welts.xyz/2021/09/30/lasso/)

[5] [【西瓜书】 第11章 特征选择与稀疏学习笔记](https://www.jianshu.com/p/0987a8841573)

[6] [利普希茨连续的几何意义是什么？怎么较好的理解它呢？ - cielo的回答 - 知乎](https://www.zhihu.com/question/51809602/answer/151391605) 

[7] [L1范数中为什么需要证明目标函数的梯度满足Lipschitz（利普希茨）连续？ - Spectre的回答 - 知乎](https://www.zhihu.com/question/620740222/answer/3531321893)

[8] [【机器学习】【白板推导系列】【合集 1～33】](https://www.bilibili.com/video/BV1aE411o7qd/)
