# 相关性分析与正态性检验

## 相关性分析

### Pearson皮尔逊相关系数

​		对于两组总体数据$X:\{X_1,X_2,\cdots,X_n\}$和$Y:\{Y_1,Y_2,\cdots Y_n\}$，其总体均值为：
$$
E(X)=\frac{\sum_{i=1}^nX_i}{n},\\
E(Y)=\frac{\sum_{i=0}^nY_i}{n}
\tag{1}.
$$
​		因此总体Covariance协方差为：
$$
\text{Cov}(X,Y)=\frac{\sum_{i=1}^n(X_i-E(X))(Y_i-E(Y))}{n},\tag{2}
$$
协方差可以通俗的理解为：两个变量在变化过程中是同方向变化，还是反方向变化，同向或反向程度如何[[1]](#1)。

​		总体皮尔逊相关系数定义为：
$$
\rho_{XY}=\frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}=\frac{\sum_{i=1}^n{\frac{X_i-E(X)}{\sigma_X}\frac{Y_i-E(Y)}{\sigma_Y}}}{n},\tag{3}
$$
上式中，$\sigma_X$和$\sigma_Y$分别表示总体数据$X$和$Y$的标准差，其计算公式为：
$$
\sigma_X=\sqrt{\frac{\sum_{i=1}^n(X-E(X))^2}{n}},\\
\sigma_Y=\sqrt{\frac{\sum_{i=1}^n(Y-E(Y))^2}{n}}.\tag{4}
$$
​		皮尔逊相关系数也可以看成是剔除了两个变量量纲影响，即将$X$和$Y$标准化后的协方差。当$X$和$Y$不是总体数据而是样本数据时，其计算同理。对于皮尔逊相关系数取值的含义如图1所示[[2]](#2)，图中的$r$即位皮尔逊相关系数值。

<img src="C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104145334771.png" alt="image-20250104145334771" style="zoom:50%;" />

<center>
    图1 维基百科Strength of Correlation
</center>

​		当然，对于计算出来的相关系数是否有效，要看每个相关系数对应的$p$值是否显著才行。需要注意的是，皮尔逊相关系数只能用来衡量两个变量的线性相关程度，其适用场景为**连续数据**，**正态分布**，**线性关系**，这三个条件均满足才能使用皮尔逊相关系数，否则就用斯皮尔曼相关系数[[3]](#3)。

### Spearman斯皮尔曼相关系数

​		斯皮尔曼相关系数也被视为是皮尔逊相关系数的一种变形，即它先把原始数据变成对应的秩（排序），然后再计算这些秩之间的皮尔逊相关系数[[4]](#4)。斯皮尔曼相关系数也被称为斯皮尔曼值秩相关系数，其适用条件比皮尔逊相关系数要广，只要数据满足**单调关系**（例如线性函数、指数函数、对数函数等）就能够使用，当数据不满足皮尔逊相关系数的要求时，如离散变量之间，或离散与连续变量之间，人们常常使用斯皮尔曼相关性系数进行计算。

​		对于两个随机变量的取值$\{X_i\}_{i=1}^n$和$\{Y_i\}_{i=1}^n$，对其从小到大进行排序，并将最小的值对应秩为1，第二小的值对应秩为2，依此类推，直到最大的值对应秩$n$。若有相同值，可以将这些相同值的秩取平均后再赋给它们（这是处理**并列秩**的方法）[[4]](#4)。

​		将样本$X_i$的秩$R(X_i)$和样本$Y_i$的秩$R(Y_i)$代入公式(3)可得斯皮尔曼相关系数的计算公式：
$$
\rho = \frac{\sum_{i=1}^{n} (R(X_i) - \overline{R(X)})(R(Y_i) - \overline{R(Y)})}{\sqrt{\sum_{i=1}^{n} (R(X_i) - \overline{R(X)})^2} \sqrt{\sum_{i=1}^{n} (R(Y_i) - \overline{R(Y)})^2}} .\tag{5}
$$
​		对于离散数据还可以具体分为两种，一种是无序的，比如学院中的物联专业、信计专业、数据专业等；另一种则是定序的，比如成绩分段中的优良中差。这两种类型的数据，斯皮尔曼相关系数都可以应对。更进一步地，对于无序数据，斯皮尔曼相关系数有一个更加简洁的计算公式：

​		 已知：
$$
\overline{R(X)} = \overline{R(Y)} = \frac{n+1}{2}.\tag{6}
$$
​		则公式(5)的分子部分可化简为：
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = \\
\sum_{i=1}^{n}R(X_i)R(Y_i) - \frac{n+1}{2}\sum_{i=1}^{n}R(X_i) - \frac{n+1}{2}\sum_{i=1}^{n}R(Y_i)+n\frac{(n+1)^2}{4}.\tag{7}
$$
由于：
$$
\sum_{i=1}^{n}R(X_i) = \sum_{i=1}^{n}R(Y_i) = \frac{n(n+1)}{2}，\tag{8}
$$
代入公式(7)可得:   
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = \sum_{i=1}^{n}R(X_i)R(Y_i) - \frac{n(n+1)^2}{4}.\tag{9}
$$
​		定义等级差$d_i = R(X_i) - R(Y_i)$，则等级差的平方和为：
$$
\sum_{i=1}^{n}d_i^2 = \sum_{i=1}^{n}(R(X_i) - R(Y_i))^2 = \sum_{i=1}^{n}(R(X_i)^2 + R(Y_i)^2 - 2R(X_i)R(Y_i)).\tag{10}
$$
​		即：
$$
\sum_{i=1}^{n}R(X_i)R(Y_i) = \frac{1}{2}\left[\sum_{i=1}^{n}R(X_i)^2 + \sum_{i=1}^{n}R(Y_i)^2 - \sum_{i=1}^{n}d_i^2\right].\tag{11}
$$
​		对于秩$R(X_i)$和$R(Y_i)$，有：
$$
\sum_{i=1}^{n}R(X_i)^2 = \sum_{i=1}^{n}R(Y_i)^2 = \frac{n(n+1)(2n+1)}{6}.\tag{12}
$$
​		结合公式(9)和(11)可将公式(5)的分子部分化简为：
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = -\frac{1}{2}\sum_{i=1}^{n}d_i^2.\tag{13}
$$
​		对于公式(5)的分母部分第一项中的根号，可以化简为：
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})^2 \\=
\sum_{i=1}^{n}R(X_i)^2 - n(\frac{n+1}{2})^2\\ = \frac{n(n+1)(2n+1)}{6} - \frac{n(n+1)^2}{4} \\= \frac{n(n^2-1)}{12}.\tag{14}
$$
​		同理，第二项中的根号，可以化简为：
$$
\sum_{i=1}^{n}(R(Y_i) - \frac{n+1}{2})^2 = \frac{n(n^2-1)}{12}.\tag{15}
$$
​		所以分母可以化简为：
$$
\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2} = \frac{n(n^2-1)}{12}.\tag{16}
$$
​		综上所述，对于无序情况下的斯皮尔曼相关系数，其计算公式可以化简为：
$$
\rho = \frac{-\frac{1}{2}\sum_{i=1}^{n}d_i^2}{\frac{n(n^2-1)}{12}}\\
=\frac{-6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}
.\tag{17}
$$
​		但是公式(17)的形式中，当$\sum_{i=1}^{n}d_i^2$为0时，计算结果为0，不符合直觉。因此通过1来调整，最终的计算公式如下：
$$
\rho = 1-\frac{6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}.\tag{18}
$$

### Kendall肯德尔相关系数

​		肯德尔相关系数适用于定序或连续变量情况下的计算，相比斯皮尔曼相关系数，更适用于小样本的场景。斯皮尔曼相关是基于秩差来进行相关关系的评估，而肯德尔相关则是基于样本数据对之间的关系来进行相关系数的强弱的分析，数据对可以分为一致对（Concordant）和分歧对（Discordant）[[5]](#5)。

​		一致对和分歧对的概念如图2所示[[6]](#6)。

<img src="C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104155900403.png" alt="image-20250104155900403" style="zoom:50%;" />

<center>
    图2 一致对和分歧对的概念
</center>


​		对于肯德尔相关系数$\tau$的计算，首先考虑所有可能的观测值对的数量：
$$
C_n^2=\frac{n(n-1)}{2}.\tag{19}
$$
​		计算一致对的数量$C$，和分歧对的数量$D$，其计算公式为：
$$
\tau=2\frac{C-D}{n(n-1)}.\tag{20}
$$

​		这里以斯皮尔曼相关系数的计算和可视化为例，绘制的热力图如图3所示。

```py
from matplotlib import rcParams

rcParams['font.family'] = 'Microsoft YaHei'
rcParams['axes.unicode_minus'] = False

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr

df = pd.read_excel(r'八年级男生体测数据.xls')

correlation_matrix, p_value_matrix = spearmanr(df)

correlation_df = pd.DataFrame(correlation_matrix, columns=df.columns, index=df.columns)
p_value_df = pd.DataFrame(p_value_matrix, columns=df.columns, index=df.columns)

plt.figure(figsize=(8, 8))
sns.heatmap(correlation_df, cmap='Blues', annot=True, fmt='.3f', vmin=-1, vmax=1)
plt.title('Spearman Correlation Heatmap of All Columns')
plt.tight_layout()
plt.savefig(r'Spearman_Correlation_Heatmap_of_All_Columns.png', dpi=600)
plt.show()

print("各列之间的 p 值矩阵：")
print(p_value_df)

p_value_df.to_excel(r'Spearman_Correlation_p_values.xlsx', sheet_name='P-Values', index=True)

```

<img src="D:\24DataMiningAssess\吴琦安_相关性差异性\Spearman Correlation Heatmap of All Columns.png" alt="Spearman Correlation Heatmap of All Columns" style="zoom: 10%;" />

<center>
    图3 斯皮尔曼相关系数热力图
</center>

​		计算的p值如图4所示：

![image-20250104213800534](C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104213800534.png)

<center>
    图4 各列的p值
</center>

## 正态性检验

### Jarque‐Bera test 雅克-贝拉检验

​		雅克-贝拉检验适用于大样本，即样本量$n\ge 30$的情况[[7]](#7)，其基本思想为：如果样本来自正态分布，那么样本的偏度和峰度应该接近正态分布的偏度和峰度。正态分布中不同偏度和峰度的可视化如图5所示，其中，通过均值来调整偏度，通过标准差来调整峰度。

![skewness_and_kurtosis](D:\24DataMiningAssess\吴琦安_相关性差异性\skewness_and_kurtosis.png)

<center>
    图5 正态分布中不同偏度和峰度的可视化
</center>


​		对于一个随机变量$X$，其总体偏度的定义为：
$$
\gamma_1=E[(\frac{X-\mu}{\sigma})^3],\tag{21}
$$
上式中，$\mu$为均值，$\sigma$为标准差。

​		在样本中，用样本均值$\bar{x}$代替总体均值$\mu$，样本标准差$s$代替总体标准差$\sigma$。因此，样本偏度$S$可用如下公式计算：
$$
S = \frac{n}{(n - 1)(n - 2)} \cdot \frac{\sum_{i=1}^{n}(x_i - \bar{x})^3}{s^3},\tag{22}
$$
上式中，$\frac{n}{(n - 1)(n - 2)}$一个修正系数，当样本量$n$较大时，这个系数趋近于1，它的存在是为了使样本偏度成为总体偏度的无偏估计量。

​		对于一个随机变量$X$，其总体峰度的定义为：
$$
\gamma_2 = E\left[\left(\frac{X - \mu}{\sigma}\right)^4\right].\tag{23}
$$
​		在样本中，样本峰度公式$K$可用如下公式计算：
$$
K = \frac{n(n + 1)}{(n - 1)(n - 2)(n - 3)} \cdot \frac{\sum_{i=1}^{n}(x_i - \bar{x})^4}{s^4} - \frac{3(n - 1)^2}{(n - 2)(n - 3)},\tag{24}
$$
​		系数$n(n + 1)/[(n - 1)(n - 2)(n - 3)]$和$-3(n - 1)^2/[(n - 2)(n - 3)]$是修正系数，它们的存在是为了使样本峰度成为总体峰度的无偏估计量。当样本量$n$较大时，这些系数的作用使得样本峰度能较好地估计总体峰度。

​		雅克贝拉检验的原假设是样本数据服从正态分布，使用Python进行雅克贝拉检验和可视化绘制如图6所示。

```py
import numpy as np
from scipy.stats import jarque_bera, gaussian_kde, skew, kurtosis
import matplotlib.pyplot as plt

# 原始数据
data = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])

# 计算原始数据的Jarque-Bera检验、偏度和峰度
statistic_data, p_value_data = jarque_bera(data)
skewness_data = skew(data)
kurt_data = kurtosis(data, fisher=True)  # Fisher’s definition of kurtosis (normal ==> 0.0)

print(f"Original Data - Skewness: {skewness_data:.4f}, Kurtosis: {kurt_data:.4f}")

# 确保所有数据都是正值，然后对数据取自然对数
log_data = np.log(data)  # 过滤掉任何可能存在的非正值

# 计算对数变换后数据的Jarque-Bera检验、偏度和峰度
statistic_log_data, p_value_log_data = jarque_bera(log_data)
skewness_log_data = skew(log_data)
kurt_log_data = kurtosis(log_data, fisher=True)  # Fisher’s definition of kurtosis (normal ==> 0.0)

print(f"Log Transformed Data - Skewness: {skewness_log_data:.4f}, Kurtosis: {kurt_log_data:.4f}")

# 创建一个2x1的子图布局
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 8.6))

# 绘制原始数据的概率密度函数
kde_data = gaussian_kde(data)
x_data = np.linspace(min(data), max(data), 1000)
axes[0].plot(x_data, kde_data(x_data), label='PDF')
axes[0].hist(data, density=True, alpha=0.5, bins=10)  # 添加直方图以对比PDF
axes[0].set_title(
    f'Original Data PDF - Jarque‐Bera Stat: {statistic_data:.4f}, P-value: {p_value_data:.4f}\nSkewness: {skewness_data:.4f}, Kurtosis: {kurt_data:.4f}')
axes[0].set_xlabel('Value')
axes[0].set_ylabel('Density')

# 绘制对数变换后数据的概率密度函数
kde_log_data = gaussian_kde(log_data)
x_log_data = np.linspace(min(log_data), max(log_data), 1000)
axes[1].plot(x_log_data, kde_log_data(x_log_data), label='PDF')
axes[1].hist(log_data, density=True, alpha=0.5, bins=10)  # 添加直方图以对比PDF
axes[1].set_title(
    f'Log Transformed Data PDF - Jarque‐Bera Stat: {statistic_log_data:.4f}, P-value: {p_value_log_data:.4f}\nSkewness: {skewness_log_data:.4f}, Kurtosis: {kurt_log_data:.4f}')
axes[1].set_xlabel('Log Value')
axes[1].set_ylabel('Density')

# 调整子图之间的间距
plt.tight_layout()

# 显示图表
plt.savefig(r'J-B test.png', dpi=600)
plt.show()
```

![J-B test](D:\24DataMiningAssess\吴琦安_相关性差异性\J-B test.png)

<center>
    图6 雅克贝拉检验及可视化
</center>

​		由图4可知，取对数前的偏度为4.0023，峰度为14.3505，p值小于0.0001，显著拒绝原假设，不服从正态分布；而在取对数之后，偏度降低至0.9789，峰度降低至1.2372，p值为0.1070，相对于取对数前接近正态分布。

### Shapiro‐wilk 夏皮洛‐威尔克检验

​		夏皮洛‐威尔克检验适用于小样本，即样本量$3\le n\le 50$的情况[[8]](#8)，其统计量为：
$$
W=\frac{(\sum_{i=1}^n\alpha_ix_{(i)})^2}{\sum_{i=1}^n(x_i-\bar x)^2},\tag{25}
$$
上式中，$x_{(i)}$表示抽样样本$x_1,x_2,\cdots x_n$从小到大排序得到顺序统计量，$\bar x$表示样本均值，系数$\alpha_i$由如下公式计算：
$$
(\alpha_1,\alpha_2,\cdots,\alpha_n)=\frac{m^TV^{-1}}{C}.\tag{26}
$$
​		假设从标准正态分布$N(0,1)$中抽取$n$个样本$x_1,x_2,\cdots x_n$，$m=(m_1,m_2,\cdots,m_n)$由从标准正态分布中独立同分布抽样得到的顺序统计量（order statistics）的期望值组成，即$m_i=E(x_{(i)})$；$V$表示正态顺序统计量（normal order statistics）的协方差矩阵，即$V_{ij}=\text{Cov}(x_{(i)},x_{(j)})$；$C$是向量范数，由如下公式计算：
$$
C = \| V^{-1} m \| = (m^T V^{-1} V^{-1} m)^{1/2}.\tag{27}
$$

​		Shapiro‐wilk检验的原假设是样本数据符合正态分布，使用Python进行Shapiro-Wilk检验：

```py
import numpy as np
from scipy.stats import shapiro

data = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])

# 进行 Shapiro-Wilk 检验
statistic, p_value = shapiro(data)

print(f"Shapiro-Wilk 统计量: {statistic}")
print(f"P 值: {p_value}")
```

​		代码输出结果为：

```py
Shapiro-Wilk 统计量: 0.3246904615503522
P 值: 1.163381439796133e-08
```

​		因此可以在99%的显著水平下拒绝原假设，该样本数据不服从正态分布。

### Kolmogorov-Smirnov 柯尔莫哥洛夫-斯米尔诺夫检验

​		K-S检验是一种非参数检验方法，主要用于比较一个样本的经验分布函数和一个理论分布函数（比如正态分布、均匀分布等），或者用于比较两个独立样本的经验分布函数是否来自同一分布。下面以博客园Arkenstone[[9]](#9)的博客为例介绍经验分布函数与理论分布函数的比较。

​		现有两列样本：

```py
controlB = {1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94, 0.38}
treatmentB= {2.37, 2.16, 14.82, 1.73, 41.04, 0.23, 1.32, 2.91, 39.41, 0.11, 27.44, 4.51, 0.51, 4.50, 0.18, 14.68, 4.66, 1.30, 2.06, 1.19}
```

​		首先分别对controlB和treatmentB进行升序，经验分布函数是对样本数据中小于等于给定值的样本点所占比例的一种累积函数，它通过对样本数据进行排序和计算累积频率来近似表示总体的分布情况，两列样本取对数前后的经验分布函数可视化如图7所示。![K-S Empirical CDF](D:\24DataMiningAssess\吴琦安_相关性差异性\K-S Empirical CDF.png)

<center>
    图7 经验分布函数可视化
</center>

​		K-S检验使用的是两条累计分布曲线之间的最大垂直差作为统计量$D_n$，其计算公式如下：
$$
D_n = \sup_x |F_n(x) - F(x)|.\tag{28}
$$
​		图4的K-S统计量出现在$x=1$附近，取值为0.45（0.65-0.25）。K-S检验的原假设是两个样本数据的分布之间没有显著差异，使用Python计算的统计量和P值：

```py
import numpy as np
from scipy.stats import ks_2samp

# 定义两个样本
controlB = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])
treatmentB = np.array(
    [2.37, 2.16, 14.82, 1.73, 41.04, 0.23, 1.32, 2.91, 39.41, 0.11, 27.44, 4.51, 0.51, 4.50, 0.18, 14.68, 4.66, 1.30,
     2.06, 1.19])

# 进行 K-S 检验
statistic, p_value = ks_2samp(controlB, treatmentB)

print(f"K-S 统计量: {statistic}")
print(f"P 值: {p_value}")
```

​		代码输出结果为：

```py
K-S 统计量: 0.45
P 值: 0.0335416594061465	
```

​		因此在95%的置信水平下显著，可以拒绝原假设，这两列样本数据的分布有显著差异。

### Lilliefors 利利福斯检验

​		Lilliefors检验在小样本情况下相对更稳健，它是K-S检验的改进版本，其原假设为样本数据服从一般正态分布，应用。K-S检验的理论分布是确定的，比如为标准正态分布。而Lilliefors检验的理论分布并不确定，比如服从某个均值和标准差下的正态分布。

### Anderson-Darling 安德森-达林检验

​		A-D检验基于累积分布函数与样本数据的经验累积分布函数之间的差异检验数据是否符合某种特定分布，其原假设为服从正态分布。连续问题上的A-D检验统计量如下：
$$
A^2 = n \int_{-\infty}^{\infty} \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))} dF(x).\tag{29}
$$
​		K-S统计量关注的是经验分布函数和理论分布函数之间的最大垂直距离，而A-D统计量考虑的是在整个分布范围内经验分布函数和理论分布函数的差异，通过积分的方式综合评估这种差异，对分布的形状差异更为敏感。故与K-S检验相比，A-D检验对尾部数据，即数据分布的极端部分更加敏感，在许多情况下比K-S检验表现得更好。

​		下面以实例展示上述五种不同的正态性检验的结果，其直方图如图8所示。

<img src="D:\24DataMiningAssess\吴琦安_相关性差异性\Lillefors test.png" alt="Lillefors test" style="zoom:15%;" />

<center>
    图8 样本数据直方图
</center>

```py
import numpy as np
from scipy import stats
from statsmodels.stats.diagnostic import lilliefors
import matplotlib.pyplot as plt
import seaborn as sns

# 数据
data = [0.23111172096338664, -1.0788096613719298, -0.35667494393873245, 0.5596157879354227, 3.9233585150918917,
        0.4382549309311553, -2.5257286443082556, -0.8675005677047231, -0.6931471805599453, 1.1631508098056809,
        -1.8971199848858813, -0.7133498878774648, -0.05129329438755058, -1.4271163556401458, 0.3148107398400336,
        -1.7719568419318752, 1.9430489167742813, -2.3025850929940455, -0.06187540371808753, -0.9675840262617056]

# 1. **Jarque-Bera (J-B) 检验**
jb_statistic, jb_p_value = stats.jarque_bera(data)
print(f"Jarque-Bera 检验统计量: {jb_statistic:.3f}, p 值: {jb_p_value:.3f}")

# 2. **Shapiro-Wilk (S-W) 检验**
sw_statistic, sw_p_value = stats.shapiro(data)
print(f"Shapiro-Wilk 检验统计量: {sw_statistic:.3f}, p 值: {sw_p_value:.3f}")

# 3. **Kolmogorov-Smirnov (K-S) 检验**
# 假设数据符合正态分布，使用均值和标准差作为参数来进行检验
ks_statistic, ks_p_value = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
print(f"Kolmogorov-Smirnov 检验统计量: {ks_statistic:.3f}, p 值: {ks_p_value:.3f}")

# 4. **Lilliefors 检验**
lilliefors_statistic, lilliefors_p_value = lilliefors(data)
print(f"Lilliefors 检验统计量: {lilliefors_statistic:.3f}, p 值: {lilliefors_p_value:.3f}")

# 5. **Anderson-Darling (A-D) 检验**
ad_result = stats.anderson(data, dist='norm')
print(f"Anderson-Darling 检验统计量: {ad_result.statistic:.3f}")

# A-D 检验的临界值和对应的显著性水平
print("Anderson-Darling 检验临界值：")
for i in range(len(ad_result.critical_values)):
    print(f"{ad_result.significance_level[i]}% 级别的临界值: {ad_result.critical_values[i]:.3f}")

# 可视化数据的直方图和正态分布拟合
sns.histplot(data, kde=True, stat='density', color='blue', label='Data Histogram')
plt.title('Data Distribution with Normal Distribution Fit')
plt.xlabel('Value')
plt.ylabel('Density')
plt.show()

```

​		代码输出结果为：

```py
Jarque-Bera 检验统计量: 4.470, p 值: 0.107
Shapiro-Wilk 检验统计量: 0.937, p 值: 0.207
Kolmogorov-Smirnov 检验统计量: 0.129, p 值: 0.853
Lilliefors 检验统计量: 0.134, p 值: 0.450
Anderson-Darling 检验统计量: 0.359
Anderson-Darling 检验临界值：
15.0% 级别的临界值: 0.506
10.0% 级别的临界值: 0.577
5.0% 级别的临界值: 0.692
2.5% 级别的临界值: 0.807
1.0% 级别的临界值: 0.960
```

​		这意味着上述Jarque-Bera、Shapiro-Wilk、Kolmogorov-Smirnov和Lilliefors检验的原假设均为服从正态分布，所有检验的p值都大于0.05，这意味着在显著性水平0.05下无法拒绝原假设，结果表明数据没有显著偏离正态分布。对于Anderson-Darling检验而言，统计量为0.359，均远小于各个临界值下的值，这说明小于所有显著性水平下的临界值，数据没有显著偏离正态分布。

## 引用

<span id="1">[1] [如何通俗易懂地解释「协方差」与「相关系数」的概念？ - GRAYLAMB的回答 - 知乎](https://www.zhihu.com/question/20852004/answer/134902061)</span>

<span id="2">[2] [Strength of Correlation](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/strength-of-correlation.html)</span>

<span id="3">[3] [数学建模：相关性分析学习——皮尔逊（pearson）相关系数与斯皮尔曼（spearman）相关系数](https://blog.csdn.net/weixin_67565775/article/details/126533149?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522a01caad53760cec50c61766855c169a6%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=a01caad53760cec50c61766855c169a6&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-126533149-null-null.142^v101^pc_search_result_base7&utm_term=%E7%9A%AE%E5%B0%94%E9%80%8A%E5%92%8C%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0&spm=1018.2226.3001.4187)</span>

<span id="4">[4] [Spearman等级相关系数 - 越来越好的文章 - 知乎](https://zhuanlan.zhihu.com/p/581986411)</span>

<span id="5">[5] [肯德尔（Kendall）相关系数概述及Python计算例](https://blog.csdn.net/chenxy_bwave/article/details/126919019)</span>

<span id="6">[6] [【时间序列分析】肯德尔（Kendall）相关系数基础理论及python代码实现](https://blog.csdn.net/qq_42761751/article/details/144523197?fromshare=blogdetail&sharetype=blogdetail&sharerId=144523197&sharerefer=PC&sharesource=L040514&sharefrom=from_link)</span>

<span id="7">[7] [【125】正态性检验 - AnkiStudy的文章 - 知乎](https://zhuanlan.zhihu.com/p/411271663)</span>

<span id="8">[8] [Shapiro–Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)</span>

<span id="9">[9] [KS-检验（Kolmogorov-Smirnov test） -- 检验数据是否符合某种分布](https://www.cnblogs.com/arkenstone/p/5496761.html)</span> 