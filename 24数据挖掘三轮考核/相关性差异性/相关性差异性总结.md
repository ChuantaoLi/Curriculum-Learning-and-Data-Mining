[TOC]

# 相关性分析

## Pearson 皮尔逊相关系数

​		对于两组总体数据$X:\{X_1,X_2,\cdots,X_n\}$和$Y:\{Y_1,Y_2,\cdots Y_n\}$，其总体均值为：
$$
E(X)=\frac{\sum_{i=1}^nX_i}{n},\\
E(Y)=\frac{\sum_{i=0}^nY_i}{n}
\tag{1}.
$$
​		因此总体Covariance协方差为：
$$
\text{Cov}(X,Y)=\frac{\sum_{i=1}^n(X_i-E(X))(Y_i-E(Y))}{n},\tag{2}
$$
协方差可以通俗的理解为：两个变量在变化过程中是同方向变化，还是反方向变化，同向或反向程度如何[[1]](#1)。

​		总体皮尔逊相关系数定义为：
$$
\rho_{XY}=\frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}=\frac{\sum_{i=1}^n{\frac{X_i-E(X)}{\sigma_X}\frac{Y_i-E(Y)}{\sigma_Y}}}{n},\tag{3}
$$
上式中，$\sigma_X$和$\sigma_Y$分别表示总体数据$X$和$Y$的标准差，其计算公式为：
$$
\sigma_X=\sqrt{\frac{\sum_{i=1}^n(X-E(X))^2}{n}},\\
\sigma_Y=\sqrt{\frac{\sum_{i=1}^n(Y-E(Y))^2}{n}}.\tag{4}
$$
​		皮尔逊相关系数也可以看成是剔除了两个变量量纲影响，即将$X$和$Y$标准化后的协方差。当$X$和$Y$不是总体数据而是样本数据时，其计算同理。对于皮尔逊相关系数取值的含义如图1所示[[2]](#2)，图中的$r$即为皮尔逊相关系数值。

<img src="C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104145334771.png" alt="image-20250104145334771" style="zoom:50%;" />

<center>
    图1 维基百科Strength of Correlation
</center>


​		当然，对于计算出来的相关系数是否有效，要看每个相关系数对应的$p$值是否显著才行。需要注意的是，皮尔逊相关系数只能用来衡量两个变量的线性相关程度，其适用场景为**连续数据**，**正态分布**，**线性关系**，这三个条件均满足才能使用皮尔逊相关系数，否则就用斯皮尔曼相关系数[[3]](#3)。

## Spearman 斯皮尔曼相关系数

​		斯皮尔曼相关系数也被视为是皮尔逊相关系数的一种变形，即它先把原始数据变成对应的秩（排序），然后再计算这些秩之间的皮尔逊相关系数[[4]](#4)。斯皮尔曼相关系数也被称为斯皮尔曼值秩相关系数，其适用条件比皮尔逊相关系数要广，只要数据满足**单调关系**（例如线性函数、指数函数、对数函数等）就能够使用，当数据不满足皮尔逊相关系数的要求时，如离散变量之间，或离散与连续变量之间，人们常常使用斯皮尔曼相关性系数进行计算。

​		对于两个随机变量的取值$\{X_i\}_{i=1}^n$和$\{Y_i\}_{i=1}^n$，对其从小到大进行排序，并将最小的值对应秩为1，第二小的值对应秩为2，依此类推，直到最大的值对应秩$n$。若有相同值，可以将这些相同值的秩取平均后再赋给它们（这是处理**并列秩**的方法）[[4]](#4)。

​		将样本$X_i$的秩$R(X_i)$和样本$Y_i$的秩$R(Y_i)$代入公式(3)可得斯皮尔曼相关系数的计算公式：
$$
\rho = \frac{\sum_{i=1}^{n} (R(X_i) - \overline{R(X)})(R(Y_i) - \overline{R(Y)})}{\sqrt{\sum_{i=1}^{n} (R(X_i) - \overline{R(X)})^2} \sqrt{\sum_{i=1}^{n} (R(Y_i) - \overline{R(Y)})^2}} .\tag{5}
$$
​		对于离散数据还可以具体分为两种，一种是无序的，比如学院中的物联专业、信计专业、数据专业等；另一种则是定序的，比如成绩分段中的优良中差。这两种类型的数据，斯皮尔曼相关系数都可以应对。更进一步地，对于无序数据，斯皮尔曼相关系数有一个更加简洁的计算公式：

​		 已知：
$$
\overline{R(X)} = \overline{R(Y)} = \frac{n+1}{2}.\tag{6}
$$
​		则公式(5)的分子部分可化简为：
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = \\
\sum_{i=1}^{n}R(X_i)R(Y_i) - \frac{n+1}{2}\sum_{i=1}^{n}R(X_i) - \frac{n+1}{2}\sum_{i=1}^{n}R(Y_i)+n\frac{(n+1)^2}{4}.\tag{7}
$$
由于：
$$
\sum_{i=1}^{n}R(X_i) = \sum_{i=1}^{n}R(Y_i) = \frac{n(n+1)}{2}，\tag{8}
$$
代入公式(7)可得:   
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = \sum_{i=1}^{n}R(X_i)R(Y_i) - \frac{n(n+1)^2}{4}.\tag{9}
$$
​		定义等级差$d_i = R(X_i) - R(Y_i)$，则等级差的平方和为：
$$
\sum_{i=1}^{n}d_i^2 = \sum_{i=1}^{n}(R(X_i) - R(Y_i))^2 = \sum_{i=1}^{n}(R(X_i)^2 + R(Y_i)^2 - 2R(X_i)R(Y_i)).\tag{10}
$$
​		即：
$$
\sum_{i=1}^{n}R(X_i)R(Y_i) = \frac{1}{2}\left[\sum_{i=1}^{n}R(X_i)^2 + \sum_{i=1}^{n}R(Y_i)^2 - \sum_{i=1}^{n}d_i^2\right].\tag{11}
$$
​		对于秩$R(X_i)$和$R(Y_i)$，有：
$$
\sum_{i=1}^{n}R(X_i)^2 = \sum_{i=1}^{n}R(Y_i)^2 = \frac{n(n+1)(2n+1)}{6}.\tag{12}
$$
​		结合公式(9)和(11)可将公式(5)的分子部分化简为：
$$
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})(R(Y_i) - \frac{n+1}{2}) = -\frac{1}{2}\sum_{i=1}^{n}d_i^2.\tag{13}
$$
​		对于公式(5)的分母部分第一项中的根号，可以化简为：
$$
\begin{align*}
\sum_{i=1}^{n}(R(X_i) - \frac{n+1}{2})^2 &= \sum_{i=1}^{n}R(X_i)^2 - n(\frac{n+1}{2})^2\\ 
&= \frac{n(n+1)(2n+1)}{6} - \frac{n(n+1)^2}{4} \\
&= \frac{n(n^2-1)}{12}
\end{align*}
.\tag{14}
$$
​		同理，第二项中的根号，可以化简为：
$$
\sum_{i=1}^{n}(R(Y_i) - \frac{n+1}{2})^2 = \frac{n(n^2-1)}{12}.\tag{15}
$$
​		所以分母可以化简为：
$$
\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2} = \frac{n(n^2-1)}{12}.\tag{16}
$$
​		综上所述，对于无序情况下的斯皮尔曼相关系数，其计算公式可以化简为：
$$
\begin{align*}
\rho &= \frac{-\frac{1}{2}\sum_{i=1}^{n}d_i^2}{\frac{n(n^2-1)}{12}}\\
&=\frac{-6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}
\end{align*}
.\tag{17}
$$
​		但是公式(17)的形式中，当$\sum_{i=1}^{n}d_i^2$为0时，计算结果为0，不符合直觉。因此通过1来调整，最终的计算公式如下：
$$
\rho = 1-\frac{6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}.\tag{18}
$$

## Kendall 肯德尔相关系数

​		肯德尔相关系数适用于定序或连续变量情况下的计算，相比斯皮尔曼相关系数，更适用于小样本的场景。斯皮尔曼相关是基于秩差来进行相关关系的评估，而肯德尔相关则是基于样本数据对之间的关系来进行相关系数的强弱的分析，数据对可以分为一致对（Concordant）和分歧对（Discordant）[[5]](#5)。

​		一致对和分歧对的概念如图2所示[[6]](#6)。

<img src="C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104155900403.png" alt="image-20250104155900403" style="zoom:50%;" />

<center>
    图2 一致对和分歧对的概念
</center>
​		对于肯德尔相关系数$\tau$的计算，首先考虑所有可能的观测值对的数量：
$$
C_n^2=\frac{n(n-1)}{2}.\tag{19}
$$
​		计算一致对的数量$C$，和分歧对的数量$D$，其计算公式为：
$$
\tau=2\frac{C-D}{n(n-1)}.\tag{20}
$$

​		这里以斯皮尔曼相关系数的计算和可视化为例，绘制的热力图如图3所示。

```py
from matplotlib import rcParams

rcParams['font.family'] = 'Microsoft YaHei'
rcParams['axes.unicode_minus'] = False

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr

df = pd.read_excel(r'八年级男生体测数据.xls')

correlation_matrix, p_value_matrix = spearmanr(df)

correlation_df = pd.DataFrame(correlation_matrix, columns=df.columns, index=df.columns)
p_value_df = pd.DataFrame(p_value_matrix, columns=df.columns, index=df.columns)

plt.figure(figsize=(8, 8))
sns.heatmap(correlation_df, cmap='Blues', annot=True, fmt='.3f', vmin=-1, vmax=1)
plt.title('Spearman Correlation Heatmap of All Columns')
plt.tight_layout()
plt.savefig(r'Spearman_Correlation_Heatmap_of_All_Columns.png', dpi=600)
plt.show()

print("各列之间的 p 值矩阵：")
print(p_value_df)

p_value_df.to_excel(r'Spearman_Correlation_p_values.xlsx', sheet_name='P-Values', index=True)

```

<img src="D:\24DataMiningAssess\吴琦安_相关性差异性\Spearman Correlation Heatmap of All Columns.png" alt="Spearman Correlation Heatmap of All Columns" style="zoom: 10%;" />

<center>
    图3 斯皮尔曼相关系数热力图
</center>


​		计算的p值如图4所示：

![image-20250104213800534](C:\Users\ChuantaoLi\AppData\Roaming\Typora\typora-user-images\image-20250104213800534.png)

<center>
    图4 各列的p值
</center>
<div style="page-break-after: always;"></div>

# 正态性检验

## Jarque‐Bera test 雅克-贝拉检验

​		雅克-贝拉检验适用于大样本，即样本量$n\ge 30$的情况[[7]](#7)，其基本思想为：如果样本来自正态分布，那么样本的偏度和峰度应该接近正态分布的偏度和峰度。正态分布中不同偏度和峰度的可视化如图5所示，其中，通过均值来调整偏度，通过标准差来调整峰度。

![skewness_and_kurtosis](D:\24DataMiningAssess\吴琦安_相关性差异性\skewness_and_kurtosis.png)

<center>
    图5 正态分布中不同偏度和峰度的可视化
</center>

​		对于一个随机变量$X$，其总体偏度的定义为：
$$
\gamma_1=E[(\frac{X-\mu}{\sigma})^3],\tag{21}
$$
上式中，$\mu$为均值，$\sigma$为标准差。

​		在样本中，用样本均值$\bar{x}$代替总体均值$\mu$，样本标准差$s$代替总体标准差$\sigma$。因此，样本偏度$S$可用如下公式计算：
$$
S = \frac{n}{(n - 1)(n - 2)} \cdot \frac{\sum_{i=1}^{n}(x_i - \bar{x})^3}{s^3},\tag{22}
$$
上式中，$\frac{n}{(n - 1)(n - 2)}$一个修正系数，当样本量$n$较大时，这个系数趋近于1，它的存在是为了使样本偏度成为总体偏度的无偏估计量。

​		对于一个随机变量$X$，其总体峰度的定义为：
$$
\gamma_2 = E\left[\left(\frac{X - \mu}{\sigma}\right)^4\right].\tag{23}
$$
​		在样本中，样本峰度公式$K$可用如下公式计算：
$$
K = \frac{n(n + 1)}{(n - 1)(n - 2)(n - 3)} \cdot \frac{\sum_{i=1}^{n}(x_i - \bar{x})^4}{s^4} - \frac{3(n - 1)^2}{(n - 2)(n - 3)},\tag{24}
$$
​		系数$n(n + 1)/[(n - 1)(n - 2)(n - 3)]$和$-3(n - 1)^2/[(n - 2)(n - 3)]$是修正系数，它们的存在是为了使样本峰度成为总体峰度的无偏估计量。当样本量$n$较大时，这些系数的作用使得样本峰度能较好地估计总体峰度。

​		雅克贝拉检验的原假设是样本数据服从正态分布，使用Python进行雅克贝拉检验和可视化绘制如图6所示。

```py
import numpy as np
from scipy.stats import jarque_bera, gaussian_kde, skew, kurtosis
import matplotlib.pyplot as plt

# 原始数据
data = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])

# 计算原始数据的Jarque-Bera检验、偏度和峰度
statistic_data, p_value_data = jarque_bera(data)
skewness_data = skew(data)
kurt_data = kurtosis(data, fisher=True)

print(f"Original Data - Skewness: {skewness_data:.4f}, Kurtosis: {kurt_data:.4f}")

# 确保所有数据都是正值，然后对数据取自然对数
log_data = np.log(data)  # 过滤掉任何可能存在的非正值

# 计算对数变换后数据的Jarque-Bera检验、偏度和峰度
statistic_log_data, p_value_log_data = jarque_bera(log_data)
skewness_log_data = skew(log_data)
kurt_log_data = kurtosis(log_data, fisher=True)

print(f"Log Transformed Data - Skewness: {skewness_log_data:.4f}, Kurtosis: {kurt_log_data:.4f}")

# 创建一个2x1的子图布局
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 8.6))

# 绘制原始数据的概率密度函数
kde_data = gaussian_kde(data)
x_data = np.linspace(min(data), max(data), 1000)
axes[0].plot(x_data, kde_data(x_data), label='PDF')
axes[0].hist(data, density=True, alpha=0.5, bins=10)  # 添加直方图以对比PDF
axes[0].set_title(
    f'Original Data PDF - Jarque‐Bera Stat: {statistic_data:.4f}, P-value: {p_value_data:.4f}\nSkewness: {skewness_data:.4f}, Kurtosis: {kurt_data:.4f}')
axes[0].set_xlabel('Value')
axes[0].set_ylabel('Density')

# 绘制对数变换后数据的概率密度函数
kde_log_data = gaussian_kde(log_data)
x_log_data = np.linspace(min(log_data), max(log_data), 1000)
axes[1].plot(x_log_data, kde_log_data(x_log_data), label='PDF')
axes[1].hist(log_data, density=True, alpha=0.5, bins=10)  # 添加直方图以对比PDF
axes[1].set_title(
    f'Log Transformed Data PDF - Jarque‐Bera Stat: {statistic_log_data:.4f}, P-value: {p_value_log_data:.4f}\nSkewness: {skewness_log_data:.4f}, Kurtosis: {kurt_log_data:.4f}')
axes[1].set_xlabel('Log Value')
axes[1].set_ylabel('Density')

# 调整子图之间的间距
plt.tight_layout()

# 显示图表
plt.savefig(r'J-B test.png', dpi=600)
plt.show()
```

![J-B test](D:\24DataMiningAssess\吴琦安_相关性差异性\J-B test.png)

<center>
    图6 雅克贝拉检验及可视化
</center>


​		由图4可知，取对数前的偏度为4.0023，峰度为14.3505，p值小于0.0001，显著拒绝原假设，不服从正态分布；而在取对数之后，偏度降低至0.9789，峰度降低至1.2372，p值为0.1070，相对于取对数前接近正态分布。

## Shapiro‐wilk 夏皮洛‐威尔克检验

​		夏皮洛‐威尔克检验适用于小样本，即样本量$3\le n\le 50$的情况[[8]](#8)，其统计量为：
$$
W=\frac{(\sum_{i=1}^n\alpha_ix_{(i)})^2}{\sum_{i=1}^n(x_i-\bar x)^2},\tag{25}
$$
上式中，$x_{(i)}$表示抽样样本$x_1,x_2,\cdots x_n$从小到大排序得到顺序统计量，$\bar x$表示样本均值，系数$\alpha_i$由如下公式计算：
$$
(\alpha_1,\alpha_2,\cdots,\alpha_n)=\frac{m^TV^{-1}}{C}.\tag{26}
$$
​		假设从标准正态分布$N(0,1)$中抽取$n$个样本$x_1,x_2,\cdots x_n$，$m=(m_1,m_2,\cdots,m_n)$由从标准正态分布中独立同分布抽样得到的顺序统计量（order statistics）的期望值组成，即$m_i=E(x_{(i)})$；$V$表示正态顺序统计量（normal order statistics）的协方差矩阵，即$V_{ij}=\text{Cov}(x_{(i)},x_{(j)})$；$C$是向量范数，由如下公式计算：
$$
C = \| V^{-1} m \| = (m^T V^{-1} V^{-1} m)^{1/2}.\tag{27}
$$

​		Shapiro‐wilk检验的原假设是样本数据符合正态分布，使用Python进行Shapiro-Wilk检验：

```py
import numpy as np
from scipy.stats import shapiro

data = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])

# 进行 Shapiro-Wilk 检验
statistic, p_value = shapiro(data)

print(f"Shapiro-Wilk 统计量: {statistic}")
print(f"P 值: {p_value}")
```

​		代码输出结果为：

```py
Shapiro-Wilk 统计量: 0.3246904615503522
P 值: 1.163381439796133e-08
```

​		因此可以在99%的显著水平下拒绝原假设，该样本数据不服从正态分布。

## Kolmogorov-Smirnov 柯尔莫哥洛夫-斯米尔诺夫检验

​		K-S检验是一种非参数检验方法，主要用于比较一个样本的经验分布函数和一个理论分布函数（比如正态分布、均匀分布等），或者用于比较两个独立样本的经验分布函数是否来自同一分布。下面以博客园Arkenstone[[9]](#9)的博客为例介绍经验分布函数与理论分布函数的比较。

​		现有两列样本：

```py
controlB = {1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94, 0.38}
treatmentB= {2.37, 2.16, 14.82, 1.73, 41.04, 0.23, 1.32, 2.91, 39.41, 0.11, 27.44, 4.51, 0.51, 4.50, 0.18, 14.68, 4.66, 1.30, 2.06, 1.19}
```

​		首先分别对controlB和treatmentB进行升序，经验分布函数是对样本数据中小于等于给定值的样本点所占比例的一种累积函数，它通过对样本数据进行排序和计算累积频率来近似表示总体的分布情况，两列样本取对数前后的经验分布函数可视化如图7所示。![K-S Empirical CDF](D:\24DataMiningAssess\吴琦安_相关性差异性\K-S Empirical CDF.png)

<center>
    图7 经验分布函数可视化
</center>
​		K-S检验使用的是两条累计分布曲线之间的最大垂直差作为统计量$D_n$，其计算公式如下：
$$
D_n = \sup_x |F_n(x) - F(x)|.\tag{28}
$$
​		图4的K-S统计量出现在$x=1$附近，取值为0.45（0.65-0.25）。K-S检验的原假设是两个样本数据的分布之间没有显著差异，使用Python计算的统计量和P值：

```py
import numpy as np
from scipy.stats import ks_2samp

# 定义两个样本
controlB = np.array(
    [1.26, 0.34, 0.70, 1.75, 50.57, 1.55, 0.08, 0.42, 0.50, 3.20, 0.15, 0.49, 0.95, 0.24, 1.37, 0.17, 6.98, 0.10, 0.94,
     0.38])
treatmentB = np.array(
    [2.37, 2.16, 14.82, 1.73, 41.04, 0.23, 1.32, 2.91, 39.41, 0.11, 27.44, 4.51, 0.51, 4.50, 0.18, 14.68, 4.66, 1.30,
     2.06, 1.19])

# 进行 K-S 检验
statistic, p_value = ks_2samp(controlB, treatmentB)

print(f"K-S 统计量: {statistic}")
print(f"P 值: {p_value}")
```

​		代码输出结果为：

```py
K-S 统计量: 0.45
P 值: 0.0335416594061465	
```

​		因此在95%的置信水平下显著，可以拒绝原假设，这两列样本数据的分布有显著差异。

## Lilliefors 利利福斯检验

​		Lilliefors检验在小样本情况下相对更稳健，它是K-S检验的改进版本，其原假设为样本数据服从一般正态分布，应用。K-S检验的理论分布是确定的，比如为标准正态分布。而Lilliefors检验的理论分布并不确定，比如服从某个均值和标准差下的正态分布。

## Anderson-Darling 安德森-达林检验

​		A-D检验基于累积分布函数与样本数据的经验累积分布函数之间的差异检验数据是否符合某种特定分布，其原假设为服从正态分布。连续问题上的A-D检验统计量如下：
$$
A^2 = n \int_{-\infty}^{\infty} \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))} dF(x).\tag{29}
$$
​		K-S统计量关注的是经验分布函数和理论分布函数之间的最大垂直距离，而A-D统计量考虑的是在整个分布范围内经验分布函数和理论分布函数的差异，通过积分的方式综合评估这种差异，对分布的形状差异更为敏感。故与K-S检验相比，A-D检验对尾部数据，即数据分布的极端部分更加敏感，在许多情况下比K-S检验表现得更好。（==这个检验我没学会==）

​		下面以实例展示上述五种不同的正态性检验的结果，其直方图如图8所示。

<img src="D:\24DataMiningAssess\吴琦安_相关性差异性\Lillefors test.png" alt="Lillefors test" style="zoom:15%;" />

<center>
    图8 样本数据直方图
</center>


```py
import numpy as np
from scipy import stats
from statsmodels.stats.diagnostic import lilliefors
import matplotlib.pyplot as plt
import seaborn as sns

# 数据
data = [0.23111172096338664, -1.0788096613719298, -0.35667494393873245, 0.5596157879354227, 3.9233585150918917,
        0.4382549309311553, -2.5257286443082556, -0.8675005677047231, -0.6931471805599453, 1.1631508098056809,
        -1.8971199848858813, -0.7133498878774648, -0.05129329438755058, -1.4271163556401458, 0.3148107398400336,
        -1.7719568419318752, 1.9430489167742813, -2.3025850929940455, -0.06187540371808753, -0.9675840262617056]

# 1. **Jarque-Bera (J-B) 检验**
jb_statistic, jb_p_value = stats.jarque_bera(data)
print(f"Jarque-Bera 检验统计量: {jb_statistic:.3f}, p 值: {jb_p_value:.3f}")

# 2. **Shapiro-Wilk (S-W) 检验**
sw_statistic, sw_p_value = stats.shapiro(data)
print(f"Shapiro-Wilk 检验统计量: {sw_statistic:.3f}, p 值: {sw_p_value:.3f}")

# 3. **Kolmogorov-Smirnov (K-S) 检验**
# 假设数据符合正态分布，使用均值和标准差作为参数来进行检验
ks_statistic, ks_p_value = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
print(f"Kolmogorov-Smirnov 检验统计量: {ks_statistic:.3f}, p 值: {ks_p_value:.3f}")

# 4. **Lilliefors 检验**
lilliefors_statistic, lilliefors_p_value = lilliefors(data)
print(f"Lilliefors 检验统计量: {lilliefors_statistic:.3f}, p 值: {lilliefors_p_value:.3f}")

# 5. **Anderson-Darling (A-D) 检验**
ad_result = stats.anderson(data, dist='norm')
print(f"Anderson-Darling 检验统计量: {ad_result.statistic:.3f}")

# A-D 检验的临界值和对应的显著性水平
print("Anderson-Darling 检验临界值：")
for i in range(len(ad_result.critical_values)):
    print(f"{ad_result.significance_level[i]}% 级别的临界值: {ad_result.critical_values[i]:.3f}")

# 可视化数据的直方图和正态分布拟合
sns.histplot(data, kde=True, stat='density', color='blue', label='Data Histogram')
plt.title('Data Distribution with Normal Distribution Fit')
plt.xlabel('Value')
plt.ylabel('Density')
plt.show()

```

​		代码输出结果为：

```py
Jarque-Bera 检验统计量: 4.470, p 值: 0.107
Shapiro-Wilk 检验统计量: 0.937, p 值: 0.207
Kolmogorov-Smirnov 检验统计量: 0.129, p 值: 0.853
Lilliefors 检验统计量: 0.134, p 值: 0.450
Anderson-Darling 检验统计量: 0.359
Anderson-Darling 检验临界值：
15.0% 级别的临界值: 0.506
10.0% 级别的临界值: 0.577
5.0% 级别的临界值: 0.692
2.5% 级别的临界值: 0.807
1.0% 级别的临界值: 0.960
```

​		这意味着上述Jarque-Bera、Shapiro-Wilk、Kolmogorov-Smirnov和Lilliefors检验的原假设均为服从正态分布，所有检验的p值都大于0.05，这意味着在显著性水平0.05下无法拒绝原假设，结果表明数据没有显著偏离正态分布。对于Anderson-Darling检验而言，统计量为0.359，均远小于各个临界值下的值，这说明小于所有显著性水平下的临界值，数据没有显著偏离正态分布。

<div style="page-break-after: always;"></div>

# 差异性分析

## Chi-squared test 卡方检验

### 卡方分布

​		卡方分布的概率密度函数的推导，知乎用户SlipLe的帖子[[10]](#10)讲解得非常清楚，甚至连推导过程需要的伽马函数和卷积操作都有公式介绍。因此，我在这里基于帖子省略的步骤进行补充。

​		卡方分布是由一组独立的标准正态分布变量的平方和所定义的。若$ Z_1, Z_2, \ldots, Z_k $是$ k $个独立的标准正态分布随机变量，即每个$ Z_i \sim N(0, 1) $，则卡方分布的随机变量$ X $定义为：
$$
X = Z_1^2 + Z_2^2 + \cdots + Z_k^2.\tag{31}
$$
​		标准正态分布$ Z \sim N(0, 1) $的概率密度函数是：
$$
f_Z(z) = \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{z^2}{2} \right), \quad -\infty < z < \infty.\tag{32}
$$
​		首先推导自由度为1的卡方分布，即当$X\sim N(0,1)$时，证明$X^2\sim \chi^2(1)$：
$$
\begin{align*}
P\{X^2 < y\} &= P\{-\sqrt{y} < X < \sqrt{y}\}\\
&=\int_{-\sqrt{y}}^{\sqrt{y}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dX\\
&=\frac{1}{2^{\frac{1}{2}} \sqrt{\pi}} y^{(\frac{1}{2}-1)} e^{-\frac{y}{2}}
\end{align*}
.\\\tag{33}
$$
​		对于伽马函数有：
$$
\Gamma(x) = \int_0^\infty e^{-t} t^{x-1} dt.\tag{34}
$$
​		为证明$\Gamma(1/2)=\sqrt{\pi}$，令$t=u^2$：
$$
\Gamma\left(\frac{1}{2}\right) = \int_0^\infty e^{-t} t^{-\frac{1}{2}} dt = 2 \int_0^\infty e^{-u^2} du,\tag{35}
$$
​		由此可知，积分项是半个高斯积分，求解高斯积分的方法是先计算$\Gamma^2(1/2)$：
$$
[\Gamma\left(\frac{1}{2}\right)]^2= 4 \int_0^{\infty} \int_0^\infty e^{-(u^2+v^2)}dudv,\tag{36}
$$
在上式中，因为$u$和$v$都是不相关的，所以可以把两个积分相乘写成二重积分的形式。

​		接下来使用极坐标变换，令$u=rcos\theta$，$v=rsin\theta$，由于$r=\sqrt{u^2+v^2}$，所以$r$的取值范围是0到$\infty$，由于$u$和$v$都是大于等于0的，因此$\theta$在第一象限，故取值范围为0到$\pi/2$，可将公式(36)化简为如下形式：
$$
[\Gamma\left(\frac{1}{2}\right)]^2=4\int_0^{\frac{\pi}{2}}\int_{0}^{\infty}e^{-r^2}rdrd\theta=\pi.\tag{37}
$$
​		因此，公式(33)可以用伽玛函数表示：
$$
P\{X^2 < y\}=\frac{(\frac{1}{2})^{\frac{1}{2}} y^{(\frac{1}{2}-1)} e^{-\frac{y}{2}}}{\Gamma(\frac{1}{2})}.\tag{38}
$$
​		现定义伽玛分布的概率密度函数：
$$
f(y, \lambda, \alpha) = \frac{\lambda^\alpha y^{(\alpha-1)} e^{-\lambda y}}{\Gamma(\alpha)}.\tag{39}
$$
​		因此，自由度为1的卡方分布的概率密度函数也可以写成如下形式：
$$
\frac{(\frac{1}{2})^{\frac{1}{2}} y^{(\frac{1}{2}-1)} e^{-\frac{y}{2}}}{\Gamma(\frac{1}{2})}=Ga(\frac{1}{2},\frac{1}{2}).\tag{40}
$$
​		下面推导自由度为$n$时的卡方分布的概率密度函数，因为涉及到多项相加，因此可以先由卷积公式证明如下性质：

​		若有$Y_1 \sim Ga(\lambda, \alpha_1)$和$Y_2 \sim Ga(\lambda, \alpha_2)$，那么$Y_1 + Y_2 \sim Ga(\lambda, \alpha_1 + \alpha_2)$。根据卷积的定义，$ Y = Y_1 + Y_2 $的概率密度函数$ f(y) $为：
$$
f(y) = f_{Y_1}(y) * f_{Y_2}(y) = \int_0^{y} \frac{\lambda^{\alpha_1} t^{\alpha_1 - 1} e^{-\lambda t}}{\Gamma(\alpha_1)} \cdot \frac{\lambda^{\alpha_2} (y - t)^{\alpha_2 - 1} e^{-\lambda (y - t)}}{\Gamma(\alpha_2)} dt.\tag{41}
$$
​		两个独立随机变量的和的概率密度函数是这两个随机变量的概率密度函数的卷积。对于连续随机变量$Y_1$和$Y_2$，$Y$的概率密度函数$f_Y(y)$由$f_{Y_1}(y)$和$f_{Y_2}(y)$的卷积给出：
$$
f_Y(y) = \int_{-\infty}^{\infty} f_{Y_1}(t) f_{Y_2}(y - t) dt.\tag{42}
$$
​	对于公式(41)，首先，将指数项合并，再将系数和幂次项合并，积分变为：
$$
f(y) =\frac{\lambda^{\alpha_1 + \alpha_2} e^{-\lambda y}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \int_0^y t^{\alpha_1 - 1} (y - t)^{\alpha_2 - 1} dt.\tag{41}
$$
​		接着对积分项进行化简，令$t=yu$，当$t=0$时有$u=0$，当$t=y$时有$u=1$，因此变量替换后的积分区域是0到1，化简后合并指数项可以得到：
$$
\int_0^y t^{\alpha_1 - 1} (y - t)^{\alpha_2 - 1} dt=\int_0^1y^{\alpha_1+\alpha_2-1}u^{\alpha_1-1}(1-u)^{\alpha_2-1}du.\tag{42}
$$
​		结合公式(42)可将公式(41)化简为如下形式：
$$
f(y) = \frac{\lambda^{\alpha_1 + \alpha_2} e^{-\lambda y} y^{\alpha_1 + \alpha_2 - 1}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \int_0^1u^{\alpha_1-1}(1-u)^{\alpha_2-1}du..\tag{42}
$$
​		对于Beta函数有如下定义：
$$
B(p, q) = \int_{0}^{1} t^{p-1}(1 - t)^{q-1} dt = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p + q)}.\tag{43}
$$
​		观察公式(42)可知，其积分项即为$B(\alpha_1,\alpha_2)$，现证明公式(43)。

​		对于公式(34)，令$t = x^2$，因此$\Gamma(p)$为：
$$
\Gamma(p)=\int_0^{\infty}e^{-x^2}(x^2)^{p-1}2xdx=2\int_0^{\infty}e^{-x^2}x^{2p-1}dx.\tag{44}
$$
​		类似的，令$t = y^2$，因此$\Gamma(q)$为：
$$
\Gamma(q)=\int_0^{\infty}e^{-y^2}(y^2)^{q-1}2ydy=2\int_0^{\infty}e^{-y^2}y^{2q-1}dy.\tag{45}
$$
​		公式(44)和公式(45)相乘可得：
$$
\Gamma(p)\Gamma(q) = 4 \int_{0}^{\infty} \int_{0}^{\infty} e^{-x^2-y^2} x^{2p-1} y^{2q-1} dxdy.\tag{46}
$$
​		利用极坐标变换，令$ x = r \cos \theta$，$y = r \sin \theta $，公式(46)可化简为：
$$
\begin{align*}
\Gamma(p)\Gamma(q)&=4\int_0^{\infty}\int_0^{\frac{\pi}{2}}e^{-r^2}(rcos\theta)^{2p-1}(rsin\theta)^{2q-1}rdrd\theta\\
&=4\int_0^{\infty}e^{-r^2}r^{2(p+q)-1}dr\int_0^{\frac{\pi}{2}}(cos\theta)^{2p-1}(sin\theta)^{2q-1}d\theta\\
&=4\int_0^{\infty}e^{-r^2}r^{2(p+q)-1}dr\int_0^{\frac{\pi}{2}}(cos^2\theta)^{p-1}\cdot cos\theta\cdot(sin^2\theta)^{q-1}\cdot sin\theta d\theta
\end{align*}
.\tag{47}
$$
​		对于$r$部分的积分，即为$\Gamma(p+q)$；对于$\theta$部分的积分，令$cos^2\theta=t$，可化简为：
$$
\int_1^{0}t^{p-1}\cdot\sqrt{t}(1-t)^{q-1}\cdot\sqrt{1-t}\cdot\frac{-1}{2\sqrt{t}\sqrt{1-t}}dt.\tag{48}
$$
​		把公式(48)中可约分的项消掉，以及负号与负号抵消后，可发现公式(48)与公式(43)形式一致，即为$B(p,q)$。

​		因此可以证明$\Gamma(p)\Gamma(q)=\Gamma(p+q)\cdot B(p,q)$。

​		至此，已经证明了若有$Y_1 \sim Ga(\lambda, \alpha_1)$和$Y_2 \sim Ga(\lambda, \alpha_2)$，那么$Y_1 + Y_2 \sim Ga(\lambda, \alpha_1 + \alpha_2)$。接下来只需要让所有的
$$
\lambda = \frac{1}{2}, \alpha_1 = \alpha_2 = \ldots = \alpha_n = \frac{1}{2}.\tag{49}
$$
​		于是：
$$
\sum_{k=1}^{n} X_k^2 + X_2^2 + \ldots + X_n^2 = \sum_{k=1}^{n} Y_1 + Y_2 + \ldots + Y_n \sim Ga\left(\frac{1}{2}, \frac{n}{2}\right).\tag{50}
$$
​		综上所述，得到卡方分布的概率密度函数：
$$
f(y) = 
\begin{cases} 
\frac{1}{2^{\frac{n}{2}} \Gamma\left(\frac{n}{2}\right)} y^{\left(\frac{n}{2} - 1\right)} e^{-\frac{y}{2}} & 0 < y \\
0 & y \leq 0 
\end{cases}.\tag{51}
$$

### 卡方检验

​		上面的推导过程是对知乎帖子“概率论-卡方分布推导[[10]](#10)”的补充，属于是基础知识的巩固了，下面介绍卡方检验的应用[[11]](#11)。

​		卡方检验（Pearson Chi-Square）的原假设是两个分类变量相互独立，是一种非参数检验，适用于总样本量大于等于40，期望频数大于等于5时的情况。假如总样本量大于等于40，有期望频数大于等于1且小于5时，选择Yates校正卡方检验（Continuity correction）。

​		卡方检验在差异性分析上的应用是列联表检验，假设有两个分类变量$ A $ 和$ B $，$ A $有$ r $个类别，$ B $有$ c $个类别。现将样本数据整理成一个$ r \times c $的列联表，其中$ n_{ij} $表示变量$ A $属于第$ i $类且变量$ B $属于第$ j $类的观察频数。

​		计算期望频数：
$$
E_{ij} = \frac{n_{i.} n_{.j}}{n},\tag{52}
$$
其中，$ n_{i.} = \sum_{j=1}^{c} n_{ij} $，$ n_{.j} = \sum_{i=1}^{r} n_{ij} $，$ n = \sum_{i=1}^{r} \sum_{j=1}^{c} n_{ij} $。

​		构造卡方检验统计量：
$$
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(n_{ij} - E_{ij})^2}{E_{ij}},\tag{53}
$$
这个统计量在两个变量相互独立的情况下近似服从自由度为$ df = (r - 1)(c - 1) $的卡方分布。

​		在进行列联表的卡方检验时，当样本量较小时，卡方分布是一个连续分布，而列联表中的频数是离散的。这种离散-连续的差异可能会导致卡方检验统计量的分布与理论卡方分布有偏差。Yates校正卡方检验主要适用于$2 \times 2$列联表，特别是当期望频数不是很大时（有期望频数大于等于1且小于5时），可以减少这种偏差，使检验结果更加准确[[12]](#12)。

​		对于一个下面这个列联表：

|      | 类别1 | 类别2 |
| :--- | :---- | :---- |
| 组1  | a     | b     |
| 组2  | c     | d     |

​		未校正的卡方统计量计算公式为：
$$
\chi^2 = \frac{(ad - bc)^2 (a + b + c + d)}{(a + b)(c + d)(a + c)(b + d)}.\tag{54}
$$
​		Yates校正后的卡方统计量计算公式为：
$$
 \chi_{Yates}^2 = \frac{(|ad - bc| - n/2)^2 (a + b + c + d)}{(a + b)(c + d)(a + c)(b + d)}, \tag{55}
$$
其中，$ n = a + b + c + d $是总样本量。

​		假设以学生性别和成绩等级为例：

|      | 优秀 | 良好 | 合格 | 不合格 |
| ---- | ---- | ---- | ---- | ------ |
| 男   | 10   | 15   | 20   | 5      |
| 女   | 8    | 12   | 18   | 2      |

```py
import numpy as np
from scipy.stats import chi2_contingency

# 构建列联表
contingency_table = np.array([[10, 15, 20, 5], [8, 12, 18, 2]])

# 未校正的卡方检验
chi2_uncorrected, p_value_uncorrected, dof_uncorrected, expected_uncorrected = chi2_contingency(contingency_table)
print("卡方统计量:", chi2_uncorrected)
print("p值:", p_value_uncorrected)
print("自由度:", dof_uncorrected)
print("期望频数:", expected_uncorrected)
```

​		其输出结果为：

```py
卡方统计量: 0.8458646616541353
p值: 0.8384682368484039
自由度: 3
期望频数: [[10 15 21.11111111 3.88888889]
 [ 8 12 16.88888889 3.11111111]]
```

​		假设显著性水平为0.05，p值远大于该显著性水平，则不拒绝原假设，认为性别和成绩等级是独立的。

​		假设上面的列联表只有2行2列：

|      | 优秀 | 良好 |
| ---- | ---- | ---- |
| 男   | 10   | 15   |
| 女   | 8    | 12   |

```py
import numpy as np
from scipy.stats import chi2_contingency

# 构建一个2x2列联表
contingency_table = np.array([[10, 15], [8, 12]])

# 未校正的卡方检验
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

print("未校正卡方统计量:", chi2)
print("未校正p值:", p_value)

# Yates校正卡方检验手动计算
a, b, c, d = contingency_table[0][0], contingency_table[0][1], contingency_table[1][0], contingency_table[1][1]
n = a + b + c + d
chi2_yates = ((np.abs(a * d - b * c) - n / 2) ** 2 * (a + b + c + d)) / ((a + b) * (c + d) * (a + c) * (b + d))

# 自由度对于2x2列联表为1
p_value_yates = 1 - np.sum(
    [np.exp(- chi2_yates / 2) * (chi2_yates / 2) ** k / np.math.factorial(k) for k in range(int(dof + 1))])
print("Yates校正卡方统计量:", chi2_yates)
print("Yates校正p值:", p_value_yates)
```

​		其输出结果为：

```py
未校正卡方统计量: 0.0
未校正p值: 1.0
Yates校正卡方统计量: 0.09375
Yates校正p值: 0.001064896563505946
```

​		由此可见，未校正的卡方检验失效。

## $t$检验

### $t$分布

​		已知$ X $服从标准正态分布$ N(0, 1) $，$ Y $服从卡方分布$ \chi^2(n) $，且$ X $与$ Y $相互独立，求$t$分布的概率密度函数即求$ T = \frac{X}{\sqrt{Y/n}} $的概率密度函数，这里的公式推导完全来源于[[13]](#13)。

​		先计算$ W = \sqrt{Y/n} $的概率密度函数$ p_W(w) $，当$w<0$时，$P(W<w)=0$；当$w\ge0$时：
$$
F_W(w)=P(W<w)=P(Y<nw^2).\tag{56}
$$
​		将上式代入公式(51)得：
$$
F_W(w)= \int_0^{nw^2} \frac{1}{2^{n/2} \Gamma(n/2)} e^{-\frac{x}{2}} x^{\frac{n}{2} - 1} dx.\tag{57}
$$
​		对$F_W(w)$求导即可得到概率密度函数$p_W(w)$：
$$
\begin{align*}
p_W(w)&= \frac{1}{2^{n/2} \Gamma(n/2)} e^{-\frac{nw^2}{2}} (nw^2)^{\frac{n}{2} - 1} \cdot 2nw\\
&=\frac{1}{\Gamma(n/2)} 2^{1 - \frac{n}{2}} n^{\frac{n}{2}} e^{-\frac{nw^2}{2}} w^{n-1}
\end{align*}
.\tag{58}
$$
​		综上所述，$ W = \sqrt{Y/n} $的概率密度函数$p_W(w)$为：
$$
p_W(w) = \begin{cases} 
\frac{1}{\Gamma(n/2)} 2^{1 - \frac{n}{2}} n^{\frac{n}{2}} e^{-\frac{nw^2}{2}} w^{n-1}, & w > 0 \\
0, & w \leq 0 
\end{cases}.\tag{59}
$$
​		因为$ X $与$ Y $相互独立，所以$ X $与$ W = \sqrt{Y/n} $也相互独立，于是$ (X, W) $的联合概率密度函数$ p(x, w) $满足：
$$
p(x, w) = p_X(x) p_W(w).\tag{60}
$$
​		对于连续型随机变量商的分布，有如下引理：

​		设$(X, Y)$为二维随机变量，其联合密度函数为$p(x, y)$，则$Z = X / Y$的密度函数$p_Z(z)$满足：

$$
p_Z(z) = \int_{-\infty}^{+\infty} p(zy, y) |y| dy.\tag{61}
$$
​		则对于$ T = \frac{X}{W} $的概率密度函数$ p_T(t) $根据引理可得：
$$
\begin{align*}
p_T(t) &= \int_{-\infty}^{+\infty} p(tw, w) |w| dw\\
&= \int_{-\infty}^{+\infty} p_X(tw) p_W(w) |w| dw\\
&= \int_0^{+\infty} p_X(tw) p_W(w) w dw\\
&= \int_0^{+\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2 w^2}{2}} \cdot \frac{1}{\Gamma(\frac{n}{2})} 2^{1 - \frac{n}{2}} n^{\frac{n}{2}} e^{-\frac{nw^2}{2}} w^{n-1} \cdot w dw\\
&= \frac{1}{\sqrt{\pi} \cdot \Gamma(\frac{n}{2})} 2^{1 - \frac{n}{2}} n^{\frac{n}{2}} \int_0^{+\infty} e^{-\frac{n + t^2}{2} w^2} w^n dw
\end{align*}.\tag{62}
$$
​		在上式中令$w=\sqrt{\frac{2z}{n+t^2}}$可得：
$$
\begin{align*}
p_T(t) &= \frac{1}{\sqrt{\pi} \cdot \Gamma(\frac{n}{2})} 2^{1 - \frac{n}{2}} n^{\frac{n}{2}} \cdot \frac{1}{2} \left( \frac{2}{n + t^2} \right)^{\frac{n+1}{2}} \int_0^{+\infty} e^{-z} z^{\frac{n-1}{2}} dz\\
&= \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi} \cdot \Gamma(\frac{n}{2})} \cdot n^{\frac{n}{2}} \left( \frac{1}{n + t^2} \right)^{\frac{n+1}{2}}\\
&= \frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi} \cdot \Gamma(\frac{n}{2})} \left( 1 + \frac{t^2}{n} \right)^{-\frac{n+1}{2}}
\end{align*}.\tag{63}
$$
​		上式即为$t$分布的概率密度函数。接下来以知乎博客"一文详解$t$检验"[[14]](#14)为基础，介绍三种$t$检验。对于下面三种未校正的$t$检验，都要求在通过方差齐性检验的前提下使用[[15]](#15)，用大白话来概括方差齐性检验，就是“就如同两个打擂的选手，重量级相差不大才能比赛，才能进行比较，否则就是欺负人，也失去了对比的意义”。除此之外，$t$检验只能应对一组或两组样本，如果要对比多组样本之间的差异性，则需要用到方差分析。主要有三中方差齐性检验方法，这里借助通义AI概述：

1. Levene 列文检验

   用于检验两个或更多个独立样本的方差是否相等，不要求数据服从正态分布。

2. Bartlett 巴特利特球形检验

   用于检验多个样本的方差是否相等，它是基于卡方分布的一种参数检验方法，因此要求数据服从或近似正态分布。

3. Fligner-Killeen 弗莱纳-基林检验

   用于检验两个或更多个独立样本的方差是否相等，它是一种非参数检验，适用于各种类型的分布。

### One-sample t-test 单样本均值检验

​		检验单样本的均值是否和已知总体的均值相等。要求总体方差未知，否则可使用$Z$检验；数据服从正态分布或近似正态，如果不服从正态分布，则使用Wilcoxon符号秩检验；数据通过方差齐性检验，如果不通过，则使用校正单样本$t$检验。单样本均值检验的例子：从某厂生产的零件中随机抽取若干件，检验其某种规格的均值是否与要求的规格相等。

​		假设有一个零件样本$ x_1, x_2, \cdots, x_n $，样本容量为$ n $，样本均值为$ \bar{x} $，总体均值的假设值为$ \mu_0 $，样本标准差为$ s $​。单样本$t$统计量的计算公式为：
$$
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}},\tag{64}
$$
上式中，$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $是样本均值，$ s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2} $是样本标准差。

​		单样本均值检验的原假设为样本均值等于已知的总体均值，$t$统计量反映了样本均值与假设的总体均值之间的差异，服从自由度为$n-1$的$t$分布。

### Independent two-sample t-test 两独立样本均值检验

​		检验两独立样本的均值是否相等。要求两样本独立，数据服从正态分布或近似正态，如果不服从正态分布，则可以使用Mann-Whitney U 检验、Wilcoxon符号秩检验或Kruskal-Wallis H检验；如果不通过方差齐性检验，则可以使用Welch-Satterthwaite公式校正的两独立样本均值检验。两独立样本均值检验的例子：需要检验两种药物对治疗高血压的效果，检验两组药物的降压水平是否相等。

​		假设有两个独立的药物样本，药物1包含$ n_1 $个观测值$ x_1, x_2, \cdots, x_{n_1} $，样本2包含$ n_2 $个观测值 $ y_1, y_2, \cdots, y_{n_2} $。样本1的均值：$ \bar{x} = \frac{1}{n_1} \sum_{i=1}^{n_1} x_i $，样本2的均值：$ \bar{y} = \frac{1}{n_2} \sum_{i=1}^{n_2} y_i $，样本1的方差：$ s_1^2 = \frac{1}{n_1 - 1} \sum_{i=1}^{n_1} (x_i - \bar{x})^2 $，样本2的方差：$ s_2^2 = \frac{1}{n_2 - 1} \sum_{i=1}^{n_2} (y_i - \bar{y})^2 $。

​		由于总体方差相等，合并方差的计算公式为：
$$
s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}.\tag{65}
$$
​		$t$统计量的计算公式为：
$$
t = \frac{\bar{x} - \bar{y}}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}.\tag{66}
$$
​		两独立样本均值检验原假设为两个独立样本来自的两个总体的均值相等。

### Dependent t-test for paired samples 配对样本均值检验

​		要求数据服从正态分布或近似正态，如果不服从正态分布，可以使用Wilcoxon符号秩检验。配对样本指的是对同一组研究对象在不同条件下进行测量或观察得到的结果，比如对一组患者进行某种治疗，对比治疗前后测量其生理指标；再比如对同卵双胞胎进行不同的实验处理，一个双胞胎接受处理A，另一个接受处理B。

​		假设有$ n $对配对数据，分别为$ (x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n) $，首先计算每对数据的差值：
$$
d_i = x_i - y_i,\quad i = 1, 2, \cdots, n \tag{67}
$$
​		接着计算差值的均值：
$$
\bar{d} = \frac{1}{n} \sum_{i=1}^{n} d_i.\tag{68}
$$
​		计算差值的标准差：
$$
s_d = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (d_i - \bar{d})^2}.\tag{69}
$$
​		则配对样本$t$统计量的计算公式为：
$$
t = \frac{\bar{d}}{s_d / \sqrt{n}}.\tag{70}
$$
​		配对样本均值检验的原假设为两组处理结果的平均差异为0，也就是两种处理在总体上没有差异。

## $F$检验

### $F$分布

​		已知$ U $服从卡方分布$ \chi^2(n_1) $，$ V $服从卡方分布$ \chi^2(n_2) $，且$ U $与$ V $相互独立，求$F$分布的概率密度函数即求$F = \frac{U/n_1}{V/n_2}$的概率密度函数，这里的公式推导完全来源于[[16]](#16)。

​		当$ w \leq 0 $时，因为$ W = Z / n = \left( \sum_{i=1}^{n} Z_i^2 \right) / n $恒为非负，所以$ P(W < w) = 0 $，从而$ p_W(w) = 0 $。下面针对$ w > 0 $的情况进行计算。
$$
\begin{align}
F_W(w) &= P(W < w)\\
&= P(Z < nw)\\
&= \int_0^{nw} \frac{1}{2^{n/2} \Gamma(n/2)} e^{-\frac{x}{2}} x^{\frac{n}{2} - 1} dx
\end{align}
.\tag{71}
$$
​		对上式求导可得：
$$
\begin{align}
p_W(w) &= F'_W(w)\\
&= \frac{1}{2^{n/2} \Gamma(n/2)} e^{-\frac{nw}{2}} (nw)^{\frac{n}{2} - 1} \cdot n\\
&= \frac{1}{2^{n/2} \Gamma(n/2)} n^{\frac{n}{2}} e^{-\frac{nw}{2}} w^{\frac{n}{2} - 1}
\end{align}
.\tag{72}
$$
​		综上所述，$ W = Z / n $的概率密度函数$p_W(w)$为：
$$
p_W(w) = \begin{cases} 
\frac{1}{2^{n/2} \Gamma(n/2)} n^{\frac{n}{2}} e^{-\frac{nw}{2}} w^{\frac{n}{2} - 1}, & w > 0 \\
0, & w \leq 0 
\end{cases}.\tag{73}
$$
​		当$t \leq 0$时，因为$ X = U / n_1 $和$ Y = V / n_2 $恒为非负，所以$ P(F < t) = P\left(\frac{X}{Y} < t\right) = 0 $，从而$ p_F(t) = 0 $。下面针对$ t > 0 $​的情况进行计算。因为$ U $与$ V $相互独立，所以$ X = U / n_1 $与$ Y = V / n_2 $也相互独立，于是$ (X, Y) $的联合概率密度函数$ p(x, y) $满足：
$$
p(x, y) = p_X(x) p_Y(y).\tag{74}
$$
​		则对于$ F = \frac{X}{Y} $的概率密度函数$ p_F(t) $根据引理可得：
$$
\begin{align}
p_F(t) &= \int_{-\infty}^{+\infty} p_X(ty) p_Y(y) |y| dy\\
&= \int_{0}^{+\infty} \frac{1}{2^{n_1/2} \Gamma(n_1/2)} n_1^{\frac{n_1}{2}} e^{-\frac{n_1 ty}{2}} (ty)^{\frac{n_1}{2} - 1} \cdot \frac{1}{2^{n_2/2} \Gamma(n_2/2)} n_2^{\frac{n_2}{2}} e^{-\frac{n_2 y}{2}} y^{\frac{n_2}{2} - 1} \cdot y dy\\
&= \frac{1}{2^{(n_1 + n_2)/2} \Gamma(n_1/2) \Gamma(n_2/2)} n_1^{\frac{n_1}{2}} n_2^{\frac{n_2}{2}} t^{\frac{n_1}{2} - 1} \left( \frac{2}{n_1 t + n_2} \right)^{\frac{n_1 + n_2}{2}} \int_{0}^{+\infty} e^{-z} z^{\frac{n_1 + n_2}{2} - 1} dz
\end{align}
.\tag{74}
$$
​		令上式中的$y = \frac{2z}{n_1 t + n_2}$，可进一步化简得：
$$
\begin{align}
p_F(t) &=\frac{\Gamma[(n_1 + n_2)/2]}{\Gamma(n_1/2) \Gamma(n_2/2)} n_1^{\frac{n_1}{2}} n_2^{\frac{n_2}{2}} t^{\frac{n_1}{2} - 1} (n_1 t + n_2)^{-\frac{n_1 + n_2}{2}}\\
&= \frac{\Gamma[(n_1 + n_2)/2]}{\Gamma(n_1/2) \Gamma(n_2/2)} \left( \frac{n_1}{n_2} \right)^{\frac{n_1}{2}} t^{\frac{n_1}{2} - 1} \left( 1 + \frac{n_1}{n_2} t \right)^{-\frac{n_1 + n_2}{2}}
\end{align}
.\tag{75}
$$
​		综上所述，$F$分布的概率密度函数为：
$$
p_F(t) = \begin{cases} 
\frac{\Gamma[(n_1 + n_2)/2]}{\Gamma(n_1/2) \Gamma(n_2/2)} \left( \frac{n_1}{n_2} \right)^{\frac{n_1}{2}} t^{\frac{n_1}{2} - 1} \left( 1 + \frac{n_1}{n_2} t \right)^{-\frac{n_1 + n_2}{2}}, & t > 0 \\
0, & t \leq 0 
\end{cases}.\tag{76}
$$
​		方差分析是$F$检验的一种应用，其用于需要比较很多组的总体均数是否有统计学差异[[17]](#17)。尽管和$t$检验一样，方差分析也用于判断样本均值是否存在显著差异，但$t$检验只能用作两组的比较，否则每多比较一次，就会增大一部分的一类错误。在假设检验中，一类错误是指当原假设实际上是正确的，但却错误地拒绝了原假设的情况。而方差分析可以克服这个问题，其目的是比较各组的均数是否均相等。

​		这里引用戴劭勍在《应用统计学与R语言实现学习笔记》中的介绍[[18]](#18)：

> ​		从引论来说，我们举个跟地学领域相关的例子。不同地貌对土壤有机质是否有影响？简单地说方差分析实质适合分析的是一系列数值型数据存在某个属性（也可以是某些），然后这个属性可以按照一定的规则分成几个类别（或者叫水平），我们想了解的就是，不同类别或者不同水平的这个数值是否存在显著性差异。简单的理解，它是处理分类型数据的。这里需要跟上一章提到的拟合优度检验、后面讲到的回归分析做些区别，拟合优度检验通常是分析两个分类变量的关系，回归分析则分析的是一个数值型变量（或多个数值型变量）对一个数值型变量的影响（或者说二者的关系）。而方差分析则是分析一个分类变量（或多个分类变量）对于一个数值变量的影响（或者说二者的关系）。

### one-way ANOVA 单因素方差分析

​		单因素方差分析用于比较一个因子下多个组的均值是否相同。例子：比较不同肥料类型对植物生长的影响，这里的因子是“肥料类型”，不同类型的肥料是组别，探究这些肥料的均值生长效果是否不同。单因素方差分析要求观察变量为连续变量，观测值相互独立，观测值可分为多组，观察变量不存在显著的异常值，各组观测值为正态（或近似正态）分布，多组观测值的整体方差相等（通过方差齐性检验）。以下笔记内容大都来源于梦特云统计的医学统计学的博客：[[19]](#19)。

​		单因素方差分析将计算好的数值代入$F$统计量即可：
$$
F=\frac{MSB}{MSW}=\frac{SSB/d_{fB}}{SSW/{d_{fW}}}=\frac{SSB/(k-1)}{SSW/(n-k)},\tag{77}
$$
上式中，$d_{fB}$为组间自由度，$d_{fW}$为组内自由度，$MSB$为组间均方，$SSB$是组间平方和，$SSW$是组内平方和，$MSW$为组内均方，$F$统计量服从$F(k-1,n-k)$。

​		假如未通过方差齐性检验，那么可以用Welch检验进行分析：
$$
F = \frac{\frac{1}{k-1} \sum_{j=1}^{k} w_j (\bar{x}_j - \bar{x}')^2}{1 + \frac{2(k-2)}{k^2-1} \sum_{j=1}^{k} \left( \frac{1}{n_j-1} \right) \left( 1 - \frac{w_j}{w} \right)^2},\tag{78}
$$
上式中，$w_j = \frac{n_j}{s_j^2}, \quad w = \sum_{j=1}^{k} w_j, \quad \bar{x}' = \frac{\sum_{j=1}^{k} w_j \bar{x}_j}{w}$，$k$为处理组数。

​		单因素方差分析整体比较，推断结论为拒绝原假设时，只能认为各总体均数之间整体比较有差异，但尚不能说明任意两个总体均数之间都有差别。若要进一步推断具体哪两个总体均数有差别，需要进一步事后检验，即多重比较[[20]](#20)。多重比较有适用于多组间两两比较的SNK法 （q检验），适用于某指定组与其它各组比较的Dunnett-t法（q'检验），以及适用于方差不齐时多组间两两比较的Games-Howell法。

### two-way ANOVA 双因素方差分析

​		双因素方差分析用于比较两个因子对响应变量的影响，可以检验两个因子的主效应，还可以检验它们的交互效应，要求观察变量唯一，且为连续变量，有两个分组变量，且都为分类变量，观测值相互独立，观察变量不存在显著的异常值，各组、各水平观察变量为正态（或近似正态）分布，相互比较的各处理水平（组别）的总体方差相等，即通过方差齐性检验。例子：研究A、B两种镇痛药物联合运用在癌症患者的镇痛效果。A药取3个剂量：1.0、2.5、5.0 mg；B药也取3个剂量：5、15、30 μg，共9个处理组。将27名研究对象随机分成9组，每组3名，记录每名对象的镇痛时间Time (min)。试分析A、B两药联合运用的镇痛效果[[21]](#21)。

​		总平方和的计算公式如下：
$$
SST = \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{n_{ij}}(X_{ijk} - \bar{X}_{...})^2,\tag{79}
$$
上式中，$ a $是因子A的水平数，$ b $ 是因子B的水平数，$ n_{ij} $ 是在第$ i $个水平的因子A和第$ j $个水平的因子B组合下的观测数，$ X_{ijk} $表示该组合下的第$ k $个观测值，$ \bar{X}_{...} $是所有观测值的总均值。

​		因子A的平方和计算公式如下：
$$
SSA = n \cdot b \cdot \sum_{i=1}^{a}(\bar{X}_{i..} - \bar{X}_{...})^2,\tag{80}
$$
上式中，$ n $是每个因子组合中的样本量，$ \bar{X}_{i..} $是因子A的第$ i $个水平的均值，因子B的平方和计算公式同理。

​		A和B交互作用的平方和计算公式如下：
$$
SSAB = n \cdot \sum_{i=1}^{a}\sum_{j=1}^{b}(\bar{X}_{ij.} - \bar{X}_{i..} - \bar{X}_{.j.} + \bar{X}_{...})^2,\tag{81}
$$
上式中，$ \bar{X}_{ij.} $是因子A的第$ i $个水平和因子B的第$ j $个水平组合的均值。

​		组内平方和的计算公式如下：
$$
SSE = SST - SSA - SSB - SSA.\tag{82}
$$
​		接着计算各因子的均方：

- 因子A的均方 (MSA)： $MSA = \frac{SSA}{a-1}$
- 因子B的均方 (MSB)：$MSB = \frac{SSB}{b-1}$
- 交互作用的均方 (MSAB)：$MSAB = \frac{SSAB}{(a-1)(b-1)}$
- 误差的均方 (MSE)：$MSE = \frac{SSE}{ab(n-1)}$

​		最后计算$F$比率：

- 因子A的F比率：$F_A = \frac{MSA}{MSE}$
- 因子B的F比率：$F_B = \frac{MSB}{MSE}$
- 交互作用的F比率：$F_{AB}=\frac{MSAB}{MSE}$

​		这里只简单介绍两种方差分析，除此之外还有协方差分析、单因素重复测量资料的方差分析、两因素重复测量资料的方差分析，后面有兴趣和时间再作补充。对于计量资料，若不满足正态和方差齐性条件，这时小样本资料选$t$检验或$F$检验是不妥的，而选秩转换的非参数检验是恰当的[[22]](#22)，下面介绍借助通义AI概述几种非参数检验方法：

1. One Sample Wilcoxon Signed Rank Test 单样本威尔科克森符号秩检验

   用于评估一个样本的中位数是否与指定的总体中位数相等，它是一个非参数版本的单样本T检验，适用于连续或有序分类数据。

2. Mann-Whitney U test 两个独立样本比较的威尔科克森秩和检验

   用于比较两个独立样本的分布是否有显著差异，它是两样本T检验的非参数替代，适用于连续或有序分类数据，不要求数据来自正态分布。

3. Paired Samples Wilcoxon Signed Rank Test 配对样本威尔科克森符号秩检验

   用于检测配对样本的中位数差异是否为零，类似于配对样本T检验，但不假定数据服从正态分布。

4. Kruskal-Wallis H Test 克鲁斯卡尔-沃尔利斯检验

   用于比较两个或更多个独立样本的分布是否有显著差异，是单因素方差分析（ANOVA）的非参数替代，适用于连续或有序分类数据。

5. The Friedman Non-parametric Repeated Measures ANOVA Test 弗里德曼检验

   用于评估多个相关样本（例如在不同时间点或不同条件下测量的同一组个体）的分布是否有显著差异，是非参数的重复测量方差分析，适用于随机区组设计的数据，即每个受试者接受所有处理条件。

# 引用

<span id="1">[1] [如何通俗易懂地解释「协方差」与「相关系数」的概念？ - GRAYLAMB的回答 - 知乎](https://www.zhihu.com/question/20852004/answer/134902061)</span>

<span id="2">[2] [Strength of Correlation](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/strength-of-correlation.html)</span>

<span id="3">[3] [数学建模：相关性分析学习——皮尔逊（pearson）相关系数与斯皮尔曼（spearman）相关系数](https://blog.csdn.net/weixin_67565775/article/details/126533149?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522a01caad53760cec50c61766855c169a6%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=a01caad53760cec50c61766855c169a6&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-126533149-null-null.142^v101^pc_search_result_base7&utm_term=%E7%9A%AE%E5%B0%94%E9%80%8A%E5%92%8C%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0&spm=1018.2226.3001.4187)</span>

<span id="4">[4] [Spearman等级相关系数 - 越来越好的文章 - 知乎](https://zhuanlan.zhihu.com/p/581986411)</span>

<span id="5">[5] [肯德尔（Kendall）相关系数概述及Python计算例](https://blog.csdn.net/chenxy_bwave/article/details/126919019)</span>

<span id="6">[6] [【时间序列分析】肯德尔（Kendall）相关系数基础理论及python代码实现](https://blog.csdn.net/qq_42761751/article/details/144523197?fromshare=blogdetail&sharetype=blogdetail&sharerId=144523197&sharerefer=PC&sharesource=L040514&sharefrom=from_link)</span>

<span id="7">[7] [【125】正态性检验 - AnkiStudy的文章 - 知乎](https://zhuanlan.zhihu.com/p/411271663)</span>

<span id="8">[8] [Shapiro–Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)</span>

<span id="9">[9] [KS-检验（Kolmogorov-Smirnov test） -- 检验数据是否符合某种分布](https://www.cnblogs.com/arkenstone/p/5496761.html)</span> 

<span id="10">[10] [概率论-卡方分布推导](https://zhuanlan.zhihu.com/p/457840913)</span> 

<span id="11">[11] [统计之美：如何优雅理解卡方分布与卡方检验之精髓所在？(重磅）](https://zhuanlan.zhihu.com/p/716236052)</span> 

<span id="12">[12] [Yates校正卡方检验](https://wenku.csdn.net/answer/2pmby0e0hr)</span> 

<span id="13">[13] [t分布概率密度函数的推导](https://blog.csdn.net/Qmj2333333/article/details/125117668?fromshare=blogdetail&sharetype=blogdetail&sharerId=125117668&sharerefer=PC&sharesource=L040514&sharefrom=from_link)</span> 

<span id="14">[14] [一文详解t检验 - CoffeeCat的文章 - 知乎](https://zhuanlan.zhihu.com/p/138711532)</span> 

<span id="15">[15] [什么是方差齐性检验（说人话）？做两独立样本T检验为什么要先做方差齐性检验？ - 不小心把你丢了的回答 - 知乎](https://www.zhihu.com/question/432195124/answer/2516199498)</span> 

<span id="16">[16] [F分布概率密度函数的推导](https://blog.csdn.net/Qmj2333333/article/details/125121901?fromshare=blogdetail&sharetype=blogdetail&sharerId=125121901&sharerefer=PC&sharesource=L040514&sharefrom=from_link)</span> 

<span id="17">[17] [如何通俗易懂地搞懂方差分析？ - 就做一只透明小白的回答 - 知乎](https://www.zhihu.com/question/293038308/answer/2624794040)</span> 

<span id="18">[18] [第 8 章 ANOVA](https://gisersqdai.top/Note-of-Applied-Statistics-with-R-Book/ANOVA.html)</span> 

<span id="19">[19] [单因素方差分析(One-way ANOVA)——理论介绍](https://mengte.online/archives/920?pcat=1)</span> 

<span id="20">[20] [北大李东风《多元统计分析》——多重比较](https://www.math.pku.edu.cn/teachers/lidf/course/mvr/mvrnotes/html/_mvrnotes/multcomp.html)</span> 

<span id="21">[21] [两因素方差分析(Two-way ANOVA)—(无交互作用)——理论介绍](https://mengte.online/archives/1210?pcat=1)</span>

<span id="22">[22] [单样本Wilcoxon符号秩检验(One Sample Wilcoxon Signed Rank Test)——理论介绍](https://mengte.online/archives/333)</span>  